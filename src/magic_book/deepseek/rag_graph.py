from dotenv import load_dotenv


# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

import traceback
from typing import Annotated, Any, Dict, List, Optional

from langchain.schema import AIMessage, HumanMessage
from langchain_core.messages import BaseMessage
from langchain_deepseek import ChatDeepSeek
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.graph.state import CompiledStateGraph
from typing_extensions import TypedDict
from loguru import logger


############################################################################################################
# é…ç½®å¸¸é‡
############################################################################################################
# ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤ï¼‰
MIN_SIMILARITY_THRESHOLD = 0.3

# æ£€ç´¢æ–‡æ¡£æ•°é‡ï¼ˆé¢„ç•™ç»™åç»­çœŸå®æ£€ç´¢ä½¿ç”¨ï¼‰
TOP_K_DOCUMENTS = 5


############################################################################################################
def _get_mock_retrieval_data(user_query: str) -> tuple[List[str], List[float]]:
    """
    ç”Ÿæˆ Mock æ£€ç´¢æ•°æ®ï¼ˆç”¨äºæµ‹è¯•RAGæµç¨‹ï¼‰

    Args:
        user_query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬

    Returns:
        (æ£€ç´¢æ–‡æ¡£åˆ—è¡¨, ç›¸ä¼¼åº¦åˆ†æ•°åˆ—è¡¨)

    Note:
        è¿™æ˜¯ä¸´æ—¶æµ‹è¯•å‡½æ•°ï¼Œåç»­ä¼šæ¥å…¥çœŸå®çš„ChromaDBæ£€ç´¢
    """
    logger.info("ğŸ­ [MOCK] ä½¿ç”¨Mockæ•°æ®æ¨¡æ‹Ÿæ£€ç´¢")

    # æ¨¡æ‹Ÿæ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—ï¼‰
    mock_docs = [
        "RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæŠ€æœ¯ï¼Œé€šè¿‡ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„å›ç­”è´¨é‡ã€‚",
        "RAGç³»ç»Ÿé€šå¸¸åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæ–‡æ¡£æ£€ç´¢å™¨ï¼ˆä½¿ç”¨å‘é‡æ•°æ®åº“å¦‚ChromaDBï¼‰ã€ä¸Šä¸‹æ–‡å¢å¼ºå™¨å’Œè¯­è¨€æ¨¡å‹ç”Ÿæˆå™¨ã€‚",
        "ä½¿ç”¨RAGæŠ€æœ¯å¯ä»¥è®©AIæ¨¡å‹è®¿é—®æœ€æ–°çš„ã€é¢†åŸŸç‰¹å®šçš„çŸ¥è¯†ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œæ˜¾è‘—æå‡å›ç­”çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ã€‚",
        "å‘é‡æ•°æ®åº“ï¼ˆå¦‚ChromaDBã€Pineconeï¼‰åœ¨RAGç³»ç»Ÿä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œå®ƒä»¬ä½¿ç”¨åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡å¹¶è¿›è¡Œè¯­ä¹‰æœç´¢ã€‚",
        "LangGraphæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæœ‰çŠ¶æ€ã€å¤šå‚ä¸è€…AIåº”ç”¨çš„æ¡†æ¶ï¼Œéå¸¸é€‚åˆå®ç°å¤æ‚çš„RAGå·¥ä½œæµã€‚",
    ]

    # æ¨¡æ‹Ÿç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆé™åºæ’åˆ—ï¼Œæ¨¡æ‹ŸçœŸå®æ£€ç´¢ç»“æœï¼‰
    mock_scores = [0.89, 0.76, 0.68, 0.52, 0.41]

    logger.info(f"ğŸ­ [MOCK] è¿”å› {len(mock_docs)} ä¸ªæ¨¡æ‹Ÿæ–‡æ¡£")
    for i, (doc, score) in enumerate(zip(mock_docs, mock_scores), 1):
        logger.debug(f"ğŸ­ [MOCK] [{i}] ç›¸ä¼¼åº¦: {score:.3f}, å†…å®¹: {doc[:50]}...")

    return mock_docs, mock_scores


############################################################################################################
class State(TypedDict, total=False):
    messages: Annotated[List[BaseMessage], add_messages]
    llm: Optional[ChatDeepSeek]  # DeepSeek LLMå®ä¾‹ï¼Œæ•´ä¸ªæµç¨‹å…±äº«


############################################################################################################
class RAGState(TypedDict, total=False):
    messages: Annotated[List[BaseMessage], add_messages]
    llm: Optional[ChatDeepSeek]  # DeepSeek LLMå®ä¾‹ï¼Œæ•´ä¸ªRAGæµç¨‹å…±äº«
    user_query: str  # ç”¨æˆ·åŸå§‹æŸ¥è¯¢
    retrieved_docs: List[str]  # æ£€ç´¢åˆ°çš„æ–‡æ¡£
    enhanced_context: str  # å¢å¼ºåçš„ä¸Šä¸‹æ–‡
    similarity_scores: List[float]  # ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰


############################################################################################################
############################################################################################################
############################################################################################################
def retrieval_node(state: RAGState) -> Dict[str, Any]:
    """
    å‘é‡æ£€ç´¢èŠ‚ç‚¹

    åŠŸèƒ½ï¼š
    1. ä½¿ç”¨ Mock æ•°æ®è¿›è¡Œæµ‹è¯•æ£€ç´¢
    2. ç›¸ä¼¼åº¦è¿‡æ»¤å’Œæ’åº
    3. å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

    Args:
        state: RAGçŠ¶æ€å¯¹è±¡

    Returns:
        åŒ…å«æ£€ç´¢ç»“æœçš„å­—å…¸ï¼š
        - user_query: ç”¨æˆ·æŸ¥è¯¢
        - retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        - similarity_scores: å¯¹åº”çš„ç›¸ä¼¼åº¦åˆ†æ•°åˆ—è¡¨

    Note:
        å½“å‰ä½¿ç”¨ Mock æ•°æ®ï¼Œåç»­ä¼šæ¥å…¥çœŸå®çš„ ChromaDB æ£€ç´¢
    """
    try:
        logger.info("ğŸ” [RETRIEVAL] å¼€å§‹å‘é‡è¯­ä¹‰æ£€ç´¢...")
        logger.info("ğŸ” [RETRIEVAL] æ£€ç´¢æ¨¡å¼: Mockæµ‹è¯•æ¨¡å¼")

        # æå–ç”¨æˆ·æŸ¥è¯¢
        user_query = state.get("user_query", "")
        if not user_query:
            # ä»æœ€æ–°æ¶ˆæ¯ä¸­æå–æŸ¥è¯¢
            if state["messages"]:
                last_message = state["messages"][-1]
                if isinstance(last_message, HumanMessage):
                    content = last_message.content
                    user_query = content if isinstance(content, str) else str(content)

        logger.info(f"ğŸ” [RETRIEVAL] ç”¨æˆ·æŸ¥è¯¢: {user_query}")

        # ä½¿ç”¨ Mock æ•°æ®è¿›è¡Œæ£€ç´¢
        retrieved_docs, similarity_scores = _get_mock_retrieval_data(user_query)

        # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
        filtered_docs = []
        filtered_scores = []

        for doc, score in zip(retrieved_docs, similarity_scores):
            if score >= MIN_SIMILARITY_THRESHOLD:
                filtered_docs.append(doc)
                filtered_scores.append(score)

        # å¦‚æœè¿‡æ»¤åæ²¡æœ‰æ–‡æ¡£ï¼Œè‡³å°‘ä¿ç•™æœ€é«˜åˆ†çš„æ–‡æ¡£
        if not filtered_docs and retrieved_docs:
            filtered_docs = [retrieved_docs[0]]
            filtered_scores = [similarity_scores[0]]
            logger.info(
                f"ğŸ” [RETRIEVAL] æ‰€æœ‰ç»“æœä½äºé˜ˆå€¼({MIN_SIMILARITY_THRESHOLD})ï¼Œ"
                f"ä¿ç•™æœ€é«˜åˆ†æ–‡æ¡£ (ç›¸ä¼¼åº¦: {similarity_scores[0]:.3f})"
            )

        logger.success(f"ğŸ” [RETRIEVAL] æ£€ç´¢å®Œæˆï¼Œå…±è¿”å› {len(filtered_docs)} ä¸ªæ–‡æ¡£")

        # è®°å½•è¯¦ç»†ä¿¡æ¯
        for i, (doc, score) in enumerate(zip(filtered_docs, filtered_scores), 1):
            logger.info(f"  ğŸ“„ [{i}] ç›¸ä¼¼åº¦: {score:.3f}, å†…å®¹: {doc[:60]}...")

        return {
            "user_query": user_query,
            "retrieved_docs": filtered_docs,
            "similarity_scores": filtered_scores,
        }

    except Exception as e:
        logger.error(f"ğŸ” [RETRIEVAL] æ£€ç´¢èŠ‚ç‚¹é”™è¯¯: {e}\n{traceback.format_exc()}")
        return {
            "user_query": state.get("user_query", ""),
            "retrieved_docs": ["æ£€ç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œå°†ä½¿ç”¨é»˜è®¤å›å¤ã€‚"],
            "similarity_scores": [0.0],
        }


############################################################################################################
def context_enhancement_node(state: RAGState) -> Dict[str, Any]:
    """
    ä¸Šä¸‹æ–‡å¢å¼ºèŠ‚ç‚¹ï¼ˆæ”¯æŒç›¸ä¼¼åº¦ä¿¡æ¯ï¼‰

    åŠŸèƒ½å¢å¼ºï¼š
    1. ä¿æŒåŸæœ‰çš„ä¸Šä¸‹æ–‡æ„å»ºé€»è¾‘
    2. æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°ä¿¡æ¯åˆ°ä¸Šä¸‹æ–‡ä¸­
    3. æä¾›æ›´ä¸°å¯Œçš„æ£€ç´¢è´¨é‡ä¿¡æ¯
    4. ä¸ºLLMæä¾›æ›´å¥½çš„å‚è€ƒä¾æ®
    """
    try:
        logger.info("ğŸ“ [ENHANCEMENT] å¼€å§‹å¢å¼ºä¸Šä¸‹æ–‡...")

        user_query = state.get("user_query", "")
        retrieved_docs = state.get("retrieved_docs", [])
        similarity_scores = state.get("similarity_scores", [])

        logger.info(f"ğŸ“ [ENHANCEMENT] å¤„ç†æŸ¥è¯¢: {user_query}")
        logger.info(f"ğŸ“ [ENHANCEMENT] æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡: {len(retrieved_docs)}")

        if similarity_scores:
            avg_similarity = sum(similarity_scores) / len(similarity_scores)
            max_similarity = max(similarity_scores)
            logger.info(
                f"ğŸ“ [ENHANCEMENT] å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}, æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}"
            )

        # æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡prompt
        context_parts = [
            "è¯·åŸºäºä»¥ä¸‹ç›¸å…³ä¿¡æ¯å›å¤ç”¨æˆ·:",
            "",
            "ç›¸å…³ä¿¡æ¯ (æŒ‰ç›¸ä¼¼åº¦æ’åº):",
        ]

        # å°†æ–‡æ¡£å’Œç›¸ä¼¼åº¦åˆ†æ•°é…å¯¹ï¼Œå¹¶æŒ‰ç›¸ä¼¼åº¦æ’åº
        if similarity_scores and len(similarity_scores) == len(retrieved_docs):
            doc_score_pairs = list(zip(retrieved_docs, similarity_scores))
            # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
            doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

            for i, (doc, score) in enumerate(doc_score_pairs, 1):
                # æ·»åŠ ç›¸ä¼¼åº¦ä¿¡æ¯åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼ˆå¸®åŠ©LLMç†è§£æ£€ç´¢è´¨é‡ï¼‰
                context_parts.append(f"{i}. [ç›¸ä¼¼åº¦: {score:.3f}] {doc}")
        else:
            # å›é€€åˆ°åŸæ¥çš„æ ¼å¼ï¼ˆæ²¡æœ‰ç›¸ä¼¼åº¦ä¿¡æ¯ï¼‰
            for i, doc in enumerate(retrieved_docs, 1):
                context_parts.append(f"{i}. {doc}")

        context_parts.extend(
            [
                "",
                f"ç”¨æˆ·é—®é¢˜: {user_query}",
                "",
                "è¯·åŸºäºä¸Šè¿°ä¿¡æ¯ç»™å‡ºå‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”:",
                "- ä¼˜å…ˆä½¿ç”¨ç›¸ä¼¼åº¦è¾ƒé«˜çš„ä¿¡æ¯",
                "- å¦‚æœç›¸ä¼¼åº¦è¾ƒä½ï¼Œè¯·é€‚å½“æé†’ç”¨æˆ·",
                "- ä¿æŒå›ç­”çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§",
            ]
        )

        enhanced_context = "\n".join(context_parts)

        logger.info("ğŸ“ [ENHANCEMENT] ä¸Šä¸‹æ–‡å¢å¼ºå®Œæˆ")
        logger.debug(f"ğŸ“ [ENHANCEMENT] å¢å¼ºåçš„ä¸Šä¸‹æ–‡:\n{enhanced_context}")

        return {"enhanced_context": enhanced_context}

    except Exception as e:
        logger.error(
            f"ğŸ“ [ENHANCEMENT] ä¸Šä¸‹æ–‡å¢å¼ºèŠ‚ç‚¹é”™è¯¯: {e}\n{traceback.format_exc()}"
        )
        fallback_context = f"è¯·å›ç­”ä»¥ä¸‹é—®é¢˜: {state.get('user_query', '')}"
        return {"enhanced_context": fallback_context}


############################################################################################################
def rag_llm_node(state: RAGState) -> Dict[str, List[BaseMessage]]:
    """RAGç‰ˆæœ¬çš„LLMèŠ‚ç‚¹"""
    try:
        logger.info("ğŸ¤– [LLM] å¼€å§‹ç”Ÿæˆå›ç­”...")

        # ä½¿ç”¨çŠ¶æ€ä¸­çš„ DeepSeek LLM å®ä¾‹
        llm = state["llm"]
        assert llm is not None, "LLM instance is None in state"

        # ä½¿ç”¨å¢å¼ºçš„ä¸Šä¸‹æ–‡æ›¿æ¢åŸå§‹æ¶ˆæ¯
        enhanced_context = state.get("enhanced_context", "")
        if enhanced_context:
            enhanced_message = HumanMessage(content=enhanced_context)
            logger.info("ğŸ¤– [LLM] ä½¿ç”¨å¢å¼ºä¸Šä¸‹æ–‡è°ƒç”¨DeepSeek")
        else:
            # å›é€€åˆ°åŸå§‹æ¶ˆæ¯ï¼Œç¡®ä¿è½¬æ¢ä¸ºHumanMessage
            if state["messages"]:
                last_msg = state["messages"][-1]
                if isinstance(last_msg, HumanMessage):
                    enhanced_message = last_msg
                else:
                    # å°†å…¶ä»–ç±»å‹çš„æ¶ˆæ¯è½¬æ¢ä¸ºHumanMessage
                    content = (
                        last_msg.content
                        if isinstance(last_msg.content, str)
                        else str(last_msg.content)
                    )
                    enhanced_message = HumanMessage(content=content)
            else:
                enhanced_message = HumanMessage(content="Hello")
            logger.warning("ğŸ¤– [LLM] å¢å¼ºä¸Šä¸‹æ–‡ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯")

        # è°ƒç”¨LLM
        response = llm.invoke([enhanced_message])
        logger.success("ğŸ¤– [LLM] DeepSeekå›ç­”ç”Ÿæˆå®Œæˆ")

        return {"messages": [response]}

    except Exception as e:
        logger.error(f"ğŸ¤– [LLM] LLMèŠ‚ç‚¹é”™è¯¯: {e}\n{traceback.format_exc()}")
        error_response = AIMessage(content="æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚")
        return {"messages": [error_response]}


############################################################################################################
def create_rag_compiled_graph() -> (
    CompiledStateGraph[RAGState, Any, RAGState, RAGState]
):
    """åˆ›å»ºRAGæµ‹è¯•ç‰ˆæœ¬çš„çŠ¶æ€å›¾"""
    logger.info("ğŸ—ï¸ æ„å»ºRAGçŠ¶æ€å›¾...")

    try:
        # åˆ›å»ºçŠ¶æ€å›¾
        graph_builder = StateGraph(RAGState)

        # æ·»åŠ ä¸‰ä¸ªèŠ‚ç‚¹
        graph_builder.add_node("retrieval", retrieval_node)
        graph_builder.add_node("enhancement", context_enhancement_node)
        graph_builder.add_node("llm", rag_llm_node)

        # è®¾ç½®èŠ‚ç‚¹æµç¨‹: retrieval â†’ enhancement â†’ llm
        graph_builder.add_edge("retrieval", "enhancement")
        graph_builder.add_edge("enhancement", "llm")

        # è®¾ç½®å…¥å£å’Œå‡ºå£ç‚¹
        graph_builder.set_entry_point("retrieval")
        graph_builder.set_finish_point("llm")

        compiled_graph = graph_builder.compile()
        logger.success("ğŸ—ï¸ RAGçŠ¶æ€å›¾æ„å»ºå®Œæˆ")

        # æ˜ç¡®ç±»å‹è½¬æ¢ä»¥æ»¡è¶³mypyè¦æ±‚
        return compiled_graph  # type: ignore[return-value]

    except Exception as e:
        logger.error(f"ğŸ—ï¸ æ„å»ºRAGçŠ¶æ€å›¾å¤±è´¥: {e}\n{traceback.format_exc()}")
        raise


############################################################################################################
def stream_rag_graph_updates(
    rag_compiled_graph: CompiledStateGraph[RAGState, Any, RAGState, RAGState],
    chat_history_state: State,
    user_input_state: State,
) -> List[BaseMessage]:
    """æ‰§è¡ŒRAGçŠ¶æ€å›¾å¹¶è¿”å›ç»“æœ"""

    ret: List[BaseMessage] = []

    try:
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡ŒRAGæµç¨‹...")

        # åˆ›å»º DeepSeek LLM å®ä¾‹
        from .client import create_deepseek_llm

        llm = create_deepseek_llm()
        logger.info("ğŸš€ åˆ›å»º DeepSeek LLM å®ä¾‹å®Œæˆ")

        # å‡†å¤‡RAGçŠ¶æ€
        user_message = (
            user_input_state["messages"][-1] if user_input_state["messages"] else None
        )
        user_query = ""
        if user_message:
            content = user_message.content
            user_query = content if isinstance(content, str) else str(content)

        rag_state: RAGState = {
            "messages": chat_history_state["messages"] + user_input_state["messages"],
            "user_query": user_query,
            "retrieved_docs": [],
            "enhanced_context": "",
            "similarity_scores": [],  # æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°å­—æ®µ
            "llm": llm,  # æ·»åŠ LLMå®ä¾‹åˆ°çŠ¶æ€ä¸­
        }

        logger.info(f"ğŸš€ RAGè¾“å…¥çŠ¶æ€å‡†å¤‡å®Œæˆï¼Œç”¨æˆ·æŸ¥è¯¢: {user_query}")

        # æ‰§è¡ŒRAGæµç¨‹
        for event in rag_compiled_graph.stream(rag_state):
            logger.debug(f"ğŸš€ RAGæµç¨‹äº‹ä»¶: {list(event.keys())}")
            for node_name, node_output in event.items():
                if "messages" in node_output:
                    ret.extend(node_output["messages"])
                    logger.info(
                        f"ğŸš€ èŠ‚ç‚¹ [{node_name}] è¾“å‡ºæ¶ˆæ¯æ•°é‡: {len(node_output['messages'])}"
                    )

        logger.success("ğŸš€ RAGæµç¨‹æ‰§è¡Œå®Œæˆ")

    except Exception as e:
        logger.error(f"ğŸš€ RAGæµç¨‹æ‰§è¡Œé”™è¯¯: {e}\n{traceback.format_exc()}")
        error_response = AIMessage(content="RAGæµç¨‹æ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚")
        ret = [error_response]

    return ret


# ############################################################################################################
# def main() -> None:
#     pass


# ############################################################################################################
# if __name__ == "__main__":
#     # æç¤ºç”¨æˆ·ä½¿ç”¨ä¸“ç”¨å¯åŠ¨è„šæœ¬
#     main()
