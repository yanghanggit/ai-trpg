#!/usr/bin/env python3
"""
Replicate å›¾åƒç”Ÿæˆå·¥å…·æ¨¡å—
åŒ…å«å¼‚æ­¥å›¾åƒç”Ÿæˆå’Œä¸‹è½½å·¥å…·
"""

import asyncio
import time
import uuid
from pathlib import Path
from typing import Dict, List, Any

import aiohttp
import replicate
from loguru import logger


# def get_default_generation_params() -> Dict[str, Any]:
#     """
#     è·å–é»˜è®¤çš„å›¾ç‰‡ç”Ÿæˆå‚æ•°

#     Returns:
#         åŒ…å«é»˜è®¤å‚æ•°çš„å­—å…¸
#     """
#     return {
#         "model_name": "sdxl-lightning",
#         "negative_prompt": "worst quality, low quality, blurry",
#         "width": 768,
#         "height": 768,
#         "num_inference_steps": 4,
#         "guidance_scale": 7.5,
#     }


def build_model_input(
    model_name: str,
    prompt: str,
    negative_prompt: str,
    width: int,
    height: int,
    num_inference_steps: int,
    guidance_scale: float,
) -> Dict[str, Any]:
    """
    æ ¹æ®ä¸åŒæ¨¡å‹æ„å»ºè¾“å…¥å‚æ•°

    Args:
        model_name: æ¨¡å‹åç§°
        prompt: æ–‡æœ¬æç¤ºè¯
        negative_prompt: è´Ÿå‘æç¤ºè¯
        width: å›¾ç‰‡å®½åº¦
        height: å›¾ç‰‡é«˜åº¦
        num_inference_steps: æ¨ç†æ­¥æ•°
        guidance_scale: å¼•å¯¼æ¯”ä¾‹

    Returns:
        æ„å»ºå¥½çš„è¾“å…¥å‚æ•°å­—å…¸

    Raises:
        ValueError: ä¸æ”¯æŒçš„æ¨¡å‹åç§°
    """
    # åŸºç¡€è¾“å…¥å‚æ•°ï¼ˆå¤§å¤šæ•°æ¨¡å‹é€šç”¨ï¼‰
    base_input = {
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "width": width,
        "height": height,
        "num_outputs": 1,
        "num_inference_steps": num_inference_steps,
        "guidance_scale": guidance_scale,
        "scheduler": "K_EULER",
    }

    # æ ¹æ®ä¸åŒæ¨¡å‹è°ƒæ•´å‚æ•°
    if model_name == "sdxl-lightning":
        # Lightning æ¨¡å‹ä½¿ç”¨è¾ƒå°‘çš„æ­¥æ•°
        base_input["num_inference_steps"] = min(4, num_inference_steps)

    elif model_name == "sdxl":
        # SDXL æ¨¡å‹ä¿æŒé»˜è®¤å‚æ•°
        pass

    elif model_name == "playground":
        # Playground æ¨¡å‹å¯èƒ½éœ€è¦ç‰¹å®šçš„è°ƒåº¦å™¨
        base_input["scheduler"] = "DPMSolverMultistep"

    elif model_name == "realvis":
        # RealVis æ¨¡å‹çš„å†™å®é£æ ¼è°ƒæ•´
        pass

    elif model_name == "ideogram-v3-turbo":
        # Ideogram V3 Turbo å¯èƒ½æœ‰ä¸åŒçš„å‚æ•°åç§°æˆ–é»˜è®¤å€¼
        pass

    else:
        # å¯¹äºæœªçŸ¥æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€å‚æ•°å¹¶è®°å½•è­¦å‘Š
        logger.warning(f"âš ï¸  æœªçŸ¥æ¨¡å‹ '{model_name}'ï¼Œä½¿ç”¨é»˜è®¤è¾“å…¥å‚æ•°")

    logger.debug(f"ğŸ”§ æ¨¡å‹ {model_name} è¾“å…¥å‚æ•°: {base_input}")
    return base_input


async def generate_image(
    prompt: str,
    model_name: str,
    negative_prompt: str,
    width: int,
    height: int,
    num_inference_steps: int,
    guidance_scale: float,
    models_config: Dict[str, Dict[str, str]],
) -> str:
    """
    å¼‚æ­¥ç”Ÿæˆå›¾ç‰‡

    Args:
        prompt: æ–‡æœ¬æç¤ºè¯
        model_name: æ¨¡å‹åç§°
        negative_prompt: è´Ÿå‘æç¤ºè¯
        width: å›¾ç‰‡å®½åº¦
        height: å›¾ç‰‡é«˜åº¦
        num_inference_steps: æ¨ç†æ­¥æ•°
        guidance_scale: å¼•å¯¼æ¯”ä¾‹
        models_config: æ¨¡å‹é…ç½®å­—å…¸

    Returns:
        å›¾ç‰‡ URL

    Raises:
        ValueError: ä¸æ”¯æŒçš„æ¨¡å‹åç§°
        Exception: å›¾ç‰‡ç”Ÿæˆå¤±è´¥
    """
    if model_name not in models_config:
        raise ValueError(
            f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}. å¯ç”¨æ¨¡å‹: {list(models_config.keys())}"
        )

    model_info = models_config[model_name]
    model_version = model_info["version"]
    cost_estimate = model_info["cost_estimate"]

    logger.info(f"ğŸ¨ ä½¿ç”¨æ¨¡å‹: {model_name}")
    logger.info(f"ğŸ’° é¢„ä¼°æˆæœ¬: {cost_estimate}")
    logger.info(f"ğŸ“ æç¤ºè¯: {prompt}")
    logger.info(f"âš™ï¸  å‚æ•°: {width}x{height}, {num_inference_steps} æ­¥")
    logger.info("ğŸ”„ å¼‚æ­¥ç”Ÿæˆä¸­...")

    start_time = time.time()

    try:
        # æ„å»ºæ¨¡å‹ç‰¹å®šçš„è¾“å…¥å‚æ•°
        model_input = build_model_input(
            model_name=model_name,
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
        )

        # ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬
        output = await replicate.async_run(model_version, input=model_input)

        # è·å–å›¾ç‰‡ URL
        image_url: str = output[0] if isinstance(output, list) else str(output)

        elapsed_time = time.time() - start_time
        logger.info(f"âœ… å¼‚æ­¥ç”Ÿæˆå®Œæˆ! è€—æ—¶: {elapsed_time:.2f}ç§’")
        logger.info(f"ğŸ”— å›¾ç‰‡ URL: {image_url}")

        return image_url

    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥ç”Ÿæˆå¤±è´¥: {e}")
        raise


async def download_image(image_url: str, save_path: str) -> str:
    """
    å¼‚æ­¥ä¸‹è½½å›¾ç‰‡

    Args:
        image_url: å›¾ç‰‡ URL
        save_path: ä¿å­˜è·¯å¾„

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„

    Raises:
        Exception: ä¸‹è½½å¤±è´¥
    """
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    save_dir = Path(save_path).parent
    save_dir.mkdir(parents=True, exist_ok=True)

    try:
        logger.info(f"ğŸ“¥ å¼‚æ­¥ä¸‹è½½å›¾ç‰‡åˆ°: {save_path}")

        # å¼‚æ­¥ä¸‹è½½å›¾ç‰‡
        async with aiohttp.ClientSession() as session:
            async with session.get(str(image_url)) as response:
                response.raise_for_status()
                content = await response.read()

        # ä¿å­˜å›¾ç‰‡
        with open(save_path, "wb") as f:
            f.write(content)

        file_size = len(content) / 1024  # KB
        logger.info(f"âœ… å¼‚æ­¥ä¸‹è½½å®Œæˆ! æ–‡ä»¶å¤§å°: {file_size:.1f} KB")

        return save_path

    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥ä¸‹è½½å¤±è´¥: {e}")
        raise


async def generate_and_download(
    prompt: str,
    model_name: str,
    negative_prompt: str,
    width: int,
    height: int,
    num_inference_steps: int,
    guidance_scale: float,
    output_dir: str,
    models_config: Dict[str, Dict[str, str]],
) -> str:
    """
    å¼‚æ­¥ç”Ÿæˆå¹¶ä¸‹è½½å›¾ç‰‡çš„ä¾¿æ·æ–¹æ³•

    Args:
        prompt: æ–‡æœ¬æç¤ºè¯
        model_name: æ¨¡å‹åç§°
        negative_prompt: è´Ÿå‘æç¤ºè¯
        width: å›¾ç‰‡å®½åº¦
        height: å›¾ç‰‡é«˜åº¦
        num_inference_steps: æ¨ç†æ­¥æ•°
        guidance_scale: å¼•å¯¼æ¯”ä¾‹
        output_dir: è¾“å‡ºç›®å½•
        models_config: æ¨¡å‹é…ç½®å­—å…¸
        filename: å¯é€‰çš„æ–‡ä»¶åï¼Œä¸åŒ…å«æ‰©å±•åã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨é»˜è®¤å‘½åè§„åˆ™

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    # å¼‚æ­¥ç”Ÿæˆå›¾ç‰‡
    image_url = await generate_image(
        prompt=prompt,
        model_name=model_name,
        negative_prompt=negative_prompt,
        width=width,
        height=height,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        models_config=models_config,
    )

    # å‡†å¤‡ä¿å­˜è·¯å¾„
    timestamp = str(uuid.uuid4())
    final_filename = f"{model_name}_{timestamp}.png"
    save_path = str(Path(output_dir) / final_filename)

    # å¼‚æ­¥ä¸‹è½½å›¾ç‰‡
    downloaded_path = await download_image(image_url, save_path)

    return downloaded_path


async def generate_multiple_images(
    prompts: List[str],
    model_name: str,
    negative_prompt: str,
    width: int,
    height: int,
    num_inference_steps: int,
    guidance_scale: float,
    output_dir: str,
    models_config: Dict[str, Dict[str, str]],
) -> List[str]:
    """
    å¹¶å‘ç”Ÿæˆå¤šå¼ å›¾ç‰‡

    Args:
        prompts: æç¤ºè¯åˆ—è¡¨
        model_name: æ¨¡å‹åç§°
        negative_prompt: è´Ÿå‘æç¤ºè¯
        width: å›¾ç‰‡å®½åº¦
        height: å›¾ç‰‡é«˜åº¦
        num_inference_steps: æ¨ç†æ­¥æ•°
        guidance_scale: å¼•å¯¼æ¯”ä¾‹
        output_dir: è¾“å‡ºç›®å½•
        models_config: æ¨¡å‹é…ç½®å­—å…¸
        filenames: å¯é€‰çš„æ–‡ä»¶ååˆ—è¡¨ï¼Œä¸åŒ…å«æ‰©å±•åã€‚å¦‚æœä¸º None æˆ–é•¿åº¦ä¸åŒ¹é…ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å‘½åè§„åˆ™

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘ç”Ÿæˆ {len(prompts)} å¼ å›¾ç‰‡...")

    # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
    tasks = []
    for i, prompt in enumerate(prompts):
        task = generate_and_download(
            prompt=prompt,
            model_name=model_name,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            output_dir=output_dir,
            models_config=models_config,
        )
        tasks.append(task)

    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    start_time = time.time()
    try:
        results = await asyncio.gather(*tasks)
        elapsed_time = time.time() - start_time
        logger.info(f"ğŸ‰ å¹¶å‘ç”Ÿæˆå®Œæˆ! æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
        logger.info(f"ğŸ“Š å¹³å‡æ¯å¼ å›¾ç‰‡: {elapsed_time/len(prompts):.2f}ç§’")
        return results
    except Exception as e:
        logger.error(f"âŒ å¹¶å‘ç”Ÿæˆå¤±è´¥: {e}")
        raise
