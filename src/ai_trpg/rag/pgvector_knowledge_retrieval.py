"""
PostgreSQL + pgvector RAGæ“ä½œæ¨¡å—

æ­¤æ¨¡å—æä¾›åŸºäº PostgreSQL + pgvector çš„ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿæ ¸å¿ƒæ“ä½œåŠŸèƒ½ï¼š
1. åˆå§‹åŒ–RAGç³»ç»Ÿ - å°†çŸ¥è¯†åº“åŠ è½½åˆ° PostgreSQL å‘é‡æ•°æ®åº“
2. è¯­ä¹‰æœç´¢ - åŸºäºæŸ¥è¯¢æ–‡æœ¬æ£€ç´¢ç›¸å…³æ–‡æ¡£

åŠŸèƒ½ï¼š
- pgvector_load_knowledge_base_to_vector_db: åˆå§‹åŒ–æ•´ä¸ªRAGç³»ç»Ÿï¼ŒåŒ…æ‹¬å‘é‡æ•°æ®åº“å’ŒçŸ¥è¯†åº“åŠ è½½
- pgvector_search_similar_documents: æ‰§è¡Œè¯­ä¹‰æœç´¢ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£å’Œç›¸ä¼¼åº¦åˆ†æ•°
"""

import traceback
from typing import Any, Dict, List, Tuple
from loguru import logger
from sentence_transformers import SentenceTransformer
from sqlalchemy import func
from ..pgsql.vector_document import (
    VectorDocumentDB,
    save_vector_document,
    search_similar_documents,
)
from ..pgsql.client import SessionLocal


############################################################################################################
# å†…éƒ¨å‡½æ•°
############################################################################################################


def _prepare_documents_for_vector_storage(
    knowledge_base: Dict[str, List[str]],
    embedding_model: SentenceTransformer,
    source: str,
) -> List[Dict[str, Any]]:
    """
    å‡†å¤‡çŸ¥è¯†åº“æ•°æ®ç”¨äºå‘é‡åŒ–å’Œå­˜å‚¨

    Args:
        knowledge_base: çŸ¥è¯†åº“æ•°æ®ï¼Œæ ¼å¼ä¸º {category: [documents]}
        embedding_model: SentenceTransformer åµŒå…¥æ¨¡å‹å®ä¾‹
        source: æ•°æ®æ¥æºæ ‡è¯†

    Returns:
        List[Dict]: å‡†å¤‡å¥½çš„æ–‡æ¡£å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«å­˜å‚¨æ‰€éœ€çš„æ‰€æœ‰å­—æ®µ
    """
    try:
        logger.info("ğŸ”„ [PREPARE] å¼€å§‹å‡†å¤‡çŸ¥è¯†åº“æ•°æ®...")

        # å‡†å¤‡æ–‡æ¡£æ•°æ®
        documents_data = []
        all_texts = []
        doc_metadata = []

        doc_id = 0
        for category, docs in knowledge_base.items():
            for doc in docs:
                all_texts.append(doc)
                doc_metadata.append(
                    {
                        "category": category,
                        "doc_id": doc_id,
                        "title": f"{category}_{doc_id}",
                    }
                )
                doc_id += 1

        logger.info(f"ğŸ“Š [PREPARE] å‡†å¤‡å‘é‡åŒ– {len(all_texts)} ä¸ªæ–‡æ¡£...")

        # ä½¿ç”¨ SentenceTransformer æ‰¹é‡è®¡ç®—å‘é‡åµŒå…¥
        logger.info("ğŸ”„ [PREPARE] è®¡ç®—æ–‡æ¡£å‘é‡åµŒå…¥...")
        embeddings = embedding_model.encode(all_texts, show_progress_bar=True)

        # ç»„è£…æ–‡æ¡£æ•°æ®
        for i, (text, embedding, metadata) in enumerate(
            zip(all_texts, embeddings, doc_metadata)
        ):
            documents_data.append(
                {
                    "content": text,
                    "embedding": embedding.tolist(),
                    "title": metadata["title"],
                    "doc_type": metadata["category"],
                    "source": source,
                    "metadata": metadata,
                }
            )

        logger.success(f"âœ… [PREPARE] æˆåŠŸå‡†å¤‡ {len(documents_data)} ä¸ªæ–‡æ¡£çš„åµŒå…¥æ•°æ®")
        logger.info(f"ğŸ“ [PREPARE] å‘é‡ç»´åº¦: {len(embeddings[0])}")

        return documents_data

    except Exception as e:
        logger.error(f"âŒ [PREPARE] å‡†å¤‡çŸ¥è¯†åº“æ•°æ®å¤±è´¥: {e}\n{traceback.format_exc()}")
        return []


############################################################################################################
# å…¬å…±å‡½æ•°
############################################################################################################


def pgvector_load_knowledge_base_to_vector_db(
    knowledge_base: Dict[str, List[str]],
    embedding_model: SentenceTransformer,
    source: str,
) -> bool:
    """
    åˆå§‹åŒ– PostgreSQL + pgvector RAGç³»ç»Ÿ

    åŠŸèƒ½ï¼š
    1. å°†çŸ¥è¯†åº“æ•°æ®å‘é‡åŒ–å¹¶å­˜å‚¨åˆ° PostgreSQL
    2. éªŒè¯ç³»ç»Ÿå°±ç»ªçŠ¶æ€

    Args:
        knowledge_base: è¦åŠ è½½çš„çŸ¥è¯†åº“æ•°æ®ï¼Œæ ¼å¼ä¸º {category: [documents]}
        embedding_model: SentenceTransformer åµŒå…¥æ¨¡å‹å®ä¾‹
        source: æ•°æ®æ¥æºæ ‡è¯†

    Returns:
        bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
    """
    logger.info("ğŸš€ [INIT] å¼€å§‹åˆå§‹åŒ– PostgreSQL + pgvector RAGç³»ç»Ÿ...")

    db = SessionLocal()
    try:
        # 1. æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰æ•°æ®
        count = db.query(func.count(VectorDocumentDB.id)).scalar()

        if count == 0:
            logger.info("ğŸ“š [INIT] æ•°æ®åº“ä¸ºç©ºï¼Œå¼€å§‹åŠ è½½çŸ¥è¯†åº“æ•°æ®...")

            # 2. å‡†å¤‡çŸ¥è¯†åº“æ•°æ®
            documents_data = _prepare_documents_for_vector_storage(
                knowledge_base, embedding_model, source
            )

            if not documents_data:
                logger.error("âŒ [INIT] çŸ¥è¯†åº“æ•°æ®å‡†å¤‡å¤±è´¥")
                return False

            # 3. æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
            logger.info("ğŸ’¾ [INIT] å­˜å‚¨å‘é‡åˆ° PostgreSQL æ•°æ®åº“...")
            saved_count = 0

            for doc_data in documents_data:
                try:
                    save_vector_document(
                        content=doc_data["content"],
                        embedding=doc_data["embedding"],
                        title=doc_data["title"],
                        doc_type=doc_data["doc_type"],
                        source=doc_data["source"],
                        metadata=doc_data["metadata"],
                    )
                    saved_count += 1
                except Exception as e:
                    logger.error(f"âŒ [INIT] ä¿å­˜æ–‡æ¡£å¤±è´¥: {e}")
                    continue

            logger.success(
                f"âœ… [INIT] æˆåŠŸåŠ è½½ {saved_count}/{len(documents_data)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"
            )

            # 4. éªŒè¯æ•°æ®åŠ è½½
            final_count = db.query(func.count(VectorDocumentDB.id)).scalar()
            logger.info(f"ğŸ“Š [INIT] æ•°æ®åº“ä¸­ç°æœ‰æ–‡æ¡£æ•°é‡: {final_count}")

        else:
            logger.info(f"â„¹ï¸ [INIT] æ•°æ®åº“ä¸­å·²æœ‰ {count} æ¡æ–‡æ¡£ï¼Œè·³è¿‡åŠ è½½")

        logger.success("ğŸ‰ [INIT] PostgreSQL + pgvector RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        return True

    except Exception as e:
        logger.error(f"âŒ [INIT] åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\n{traceback.format_exc()}")
        logger.warning("âš ï¸ [INIT] ç³»ç»Ÿå°†å›é€€åˆ°å…³é”®è¯åŒ¹é…æ¨¡å¼")
        return False
    finally:
        db.close()


############################################################################################################


def pgvector_search_similar_documents(
    query: str,
    embedding_model: SentenceTransformer,
    top_k: int,
    similarity_threshold: float = 0.3,
    doc_type_filter: str | None = None,
) -> Tuple[List[str], List[float]]:
    """
    æ‰§è¡Œè¯­ä¹‰æœç´¢

    åŠŸèƒ½ï¼š
    1. è®¡ç®—æŸ¥è¯¢å‘é‡
    2. åœ¨ PostgreSQL + pgvector ä¸­æ‰§è¡Œå‘é‡æœç´¢
    3. è¿”å›æœç´¢ç»“æœ

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
        embedding_model: SentenceTransformer åµŒå…¥æ¨¡å‹å®ä¾‹
        top_k: è¿”å›æœ€ç›¸ä¼¼çš„æ–‡æ¡£æ•°é‡
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0)
        doc_type_filter: æ–‡æ¡£ç±»å‹è¿‡æ»¤ (å¯é€‰)

    Returns:
        tuple: (æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨, ç›¸ä¼¼åº¦åˆ†æ•°åˆ—è¡¨)
    """
    try:
        logger.info(f"ğŸ” [PGVECTOR] æ‰§è¡Œè¯­ä¹‰æœç´¢: '{query}'")

        # 1. è®¡ç®—æŸ¥è¯¢å‘é‡
        query_embedding = embedding_model.encode([query])[0]
        query_embedding_list = query_embedding.tolist()

        logger.debug(f"ğŸ“ [PGVECTOR] æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_embedding_list)}")

        # 2. åœ¨ PostgreSQL ä¸­æ‰§è¡Œå‘é‡æœç´¢
        results = search_similar_documents(
            query_embedding=query_embedding_list,
            limit=top_k,
            similarity_threshold=similarity_threshold,
            doc_type_filter=doc_type_filter,
        )

        # 3. æå–ç»“æœ
        documents = [doc.content for doc, _ in results]
        similarity_scores = [score for _, score in results]

        logger.info(f"âœ… [PGVECTOR] æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£")

        # 4. æ‰“å°æœç´¢ç»“æœè¯¦æƒ…ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        for i, (doc_obj, score) in enumerate(results):
            logger.debug(
                f"  ğŸ“„ [{i+1}] ç›¸ä¼¼åº¦: {score:.3f}, ç±»åˆ«: {doc_obj.doc_type}, å†…å®¹: {doc_obj.content[:50]}..."
            )

        return documents, similarity_scores

    except Exception as e:
        logger.error(f"âŒ [PGVECTOR] è¯­ä¹‰æœç´¢å¤±è´¥: {e}\n{traceback.format_exc()}")
        return [], []


############################################################################################################
