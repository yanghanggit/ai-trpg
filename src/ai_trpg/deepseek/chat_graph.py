from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

import traceback
from typing import Annotated, Any, List, Optional
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langchain_deepseek import ChatDeepSeek
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.graph.state import CompiledStateGraph
from typing_extensions import TypedDict
from loguru import logger


############################################################################################################
class ChatState(TypedDict, total=False):
    """èŠå¤©çŠ¶æ€çš„ç±»å‹å®šä¹‰

    Attributes:
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œä½¿ç”¨ add_messages æ³¨è§£æ¥è‡ªåŠ¨å¤„ç†æ¶ˆæ¯åˆå¹¶
        llm: DeepSeek LLM å®ä¾‹ï¼Œç”¨äºç”Ÿæˆå“åº”
        llm_response: LLM ç”Ÿæˆçš„å“åº”æ¶ˆæ¯
    """

    messages: Annotated[List[BaseMessage], add_messages]
    llm: ChatDeepSeek
    llm_response: AIMessage


############################################################################################################
def _chatbot_node(
    state: ChatState,
) -> ChatState:
    """èŠå¤©æœºå™¨äººèŠ‚ç‚¹,æœ€ç®€å•çš„å®ç°"""
    llm = state["llm"]  # ä½¿ç”¨çŠ¶æ€ä¸­çš„LLMå®ä¾‹
    response = llm.invoke(state["messages"])  # ç”Ÿæˆå“åº”
    assert isinstance(response, AIMessage), "LLM è¿”å›çš„å“åº”å¿…é¡»æ˜¯ AIMessage ç±»å‹"
    return {
        "messages": [response],  # messages ä¼šé€šè¿‡ add_messages è‡ªåŠ¨åˆå¹¶åˆ°å†å²ä¸­
        "llm": llm,
        "llm_response": response,  # å•ç‹¬è®°å½•æœ€æ–°çš„å“åº”
    }


############################################################################################################
def create_chat_workflow() -> CompiledStateGraph[ChatState, Any, ChatState, ChatState]:
    """åˆ›å»ºå¹¶ç¼–è¯‘ DeepSeek èŠå¤©å›¾"""

    graph_builder = StateGraph(ChatState)
    graph_builder.add_node("chatbot_node", _chatbot_node)
    graph_builder.set_entry_point("chatbot_node")
    graph_builder.set_finish_point("chatbot_node")
    return graph_builder.compile()  # type: ignore[return-value]


############################################################################################################
def print_full_message_chain(state: ChatState) -> None:
    """
    æ‰“å°å®Œæ•´çš„æ¶ˆæ¯é“¾è·¯ï¼Œç”¨äºè°ƒè¯•å’Œè¿½è¸ªå¯¹è¯æµç¨‹

    Args:
        state: å½“å‰çŠ¶æ€
    """
    messages = state.get("messages", [])
    logger.info(f"ğŸ“œ å®Œæ•´æ¶ˆæ¯é“¾è·¯ (å…± {len(messages)} æ¡æ¶ˆæ¯)")
    for i, msg in enumerate(messages, 0):
        logger.debug(
            f"[{i}] å®Œæ•´å†…å®¹:\n{msg.model_dump_json(indent=2, ensure_ascii=False)}\n"
        )


############################################################################################################
async def execute_chat_workflow(
    work_flow: CompiledStateGraph[ChatState, Any, ChatState, ChatState],
    context: List[BaseMessage],
    request: HumanMessage,
    llm: ChatDeepSeek,
) -> List[BaseMessage]:
    """æ‰§è¡ŒèŠå¤©å·¥ä½œæµå¹¶è¿”å›æ‰€æœ‰å“åº”æ¶ˆæ¯

    å°†èŠå¤©å†å²å’Œç”¨æˆ·è¾“å…¥åˆå¹¶åï¼Œé€šè¿‡ç¼–è¯‘å¥½çš„çŠ¶æ€å›¾è¿›è¡Œæµå¼å¤„ç†ï¼Œ
    æ”¶é›†å¹¶è¿”å›æ‰€æœ‰ç”Ÿæˆçš„æ¶ˆæ¯ã€‚ChatState çš„åˆ›å»ºè¢«å°è£…åœ¨å‡½æ•°å†…éƒ¨ã€‚

    Args:
        work_flow: å·²ç¼–è¯‘çš„ LangGraph çŠ¶æ€å›¾
        context: å†å²æ¶ˆæ¯åˆ—è¡¨
        request: ç”¨æˆ·å½“å‰è¾“å…¥çš„æ¶ˆæ¯
        llm: ChatDeepSeek LLM å®ä¾‹

    Returns:
        åŒ…å«æ‰€æœ‰ç”Ÿæˆæ¶ˆæ¯çš„åˆ—è¡¨
    """
    ret: List[BaseMessage] = []

    # åœ¨å†…éƒ¨æ„é€  ChatStateï¼ˆå°è£…å®ç°ç»†èŠ‚ï¼‰
    workflow_state: ChatState = {
        "messages": context + [request],
        "llm": llm,
    }

    try:

        last_state: Optional[ChatState] = None
        async for event in work_flow.astream(workflow_state):
            for value in event.values():
                last_state = value  # è®°å½•æœ€åä¸€ä¸ª state

        # å¦‚æœå­˜åœ¨æœ€åä¸€ä¸ª state ä¸”åŒ…å« llm_responseï¼Œåˆ™è¿”å›å®ƒ
        if last_state and "llm_response" in last_state:
            assert isinstance(last_state["llm_response"], AIMessage)
            ret = [last_state["llm_response"]]

            print_full_message_chain(last_state)  # æ‰“å°å®Œæ•´æ¶ˆæ¯é“¾è·¯ç”¨äºè°ƒè¯•

    except Exception as e:
        logger.error(
            f"Error executing chat workflow: {e}\n" f"Workflow state: {workflow_state}"
        )
        traceback.print_exc()

    return ret


############################################################################################################
