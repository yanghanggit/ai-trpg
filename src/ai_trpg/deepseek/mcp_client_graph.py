"""
MCP Client Graph - åŸºäº LangGraph çš„ MCP å·¥å…·è°ƒç”¨å·¥ä½œæµ

## å·¥ä½œæµæ¶æ„
preprocess â†’ llm_invoke â†’ tool_parse â†’ [æ¡ä»¶åˆ¤æ–­]
                                          â†“ (éœ€è¦å·¥å…·)
                                    tool_execution â†’ llm_re_invoke â†’ response_synthesis
                                          â†“ (ä¸éœ€è¦å·¥å…·)
                                    response_synthesis

## ä¸Šä¸‹æ–‡å˜åŒ–é“¾è·¯ï¼ˆContext Chainï¼‰

### æ ¸å¿ƒæ”¹è¿›ï¼šäºŒæ¬¡æ¨ç†ï¼ˆRe-invokeï¼‰æ¨¡å¼
è®©AIåŸºäºå·¥å…·ç»“æœè¿›è¡Œæ·±åº¦åˆ†æï¼Œè€Œä¸æ˜¯ç®€å•æ‹¼æ¥ã€‚

### ä¸Šä¸‹æ–‡æ¼”å˜ï¼ˆ6ä¸ªé˜¶æ®µï¼‰
1. **åˆå§‹ä¸Šä¸‹æ–‡**: `context["messages"] + request["messages"]`
2. **é¢„å¤„ç†å¢å¼º**: æ·»åŠ å·¥å…·æŒ‡ä»¤ â†’ ç›´æ¥ä¿å­˜åˆ° `messages`
3. **ç¬¬ä¸€æ¬¡æ¨ç†**: LLMå†³å®šè°ƒç”¨å·¥å…· â†’ `llm_response`
4. **å·¥å…·æ‰§è¡Œ**: å¹¶å‘æ‰§è¡Œå·¥å…· â†’ `tool_outputs`
5. **äºŒæ¬¡æ¨ç†** â­: ä¿æŒå®Œæ•´å¯¹è¯å†å²ï¼Œè¿½åŠ å·¥å…·ç»“æœ
   ```
   [SystemMessage(è§’è‰²), SystemMessage(å·¥å…·è¯´æ˜),
    HumanMessage(ç”¨æˆ·é—®é¢˜1), AIMessage(AIå›ç­”1),  # â† ä¿ç•™å®Œæ•´å†å²
    HumanMessage(ç”¨æˆ·é—®é¢˜2), AIMessage(AIå›ç­”2),  # â† ä¿ç•™å®Œæ•´å†å²
    HumanMessage(å½“å‰é—®é¢˜), AIMessage(å·¥å…·è°ƒç”¨å†³ç­–),
    AIMessage(å·¥å…·æ‰§è¡Œç»“æœ)]  # â† å…³é”®ï¼šä½¿ç”¨AIMessageè€ŒéSystemMessage
   ```
6. **æœ€ç»ˆå“åº”**: åŸºäºå®Œæ•´ä¸Šä¸‹æ–‡å’Œå·¥å…·ç»“æœçš„æ™ºèƒ½å›ç­” â†’ `final_response`

### å…³é”®è®¾è®¡
- **æ¶ˆæ¯ç±»å‹**: å·¥å…·ç»“æœç”¨ `AIMessage`ï¼ˆAIçš„è§‚å¯Ÿï¼‰è€Œé `SystemMessage`
- **æ¶ˆæ¯é¡ºåº**: ç”¨æˆ·é—®é¢˜ â†’ AIå“åº” â†’ å·¥å…·ç»“æœï¼ˆç¬¦åˆå› æœå…³ç³»ï¼‰
- **è°ƒè¯•åŠŸèƒ½**: `_print_full_context_chain()` æ‰“å°å®Œæ•´é“¾è·¯ï¼ˆDEBUGçº§åˆ«ï¼‰
"""

from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

import asyncio
from typing import Annotated, Any, Dict, List, Optional
from langchain.schema import AIMessage, SystemMessage, HumanMessage
from langchain_core.messages import BaseMessage
from langchain_deepseek import ChatDeepSeek
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.graph.state import CompiledStateGraph
from typing_extensions import TypedDict
from ..mcp import (
    McpClient,
    McpToolInfo,
    ToolCallParser,
    execute_mcp_tool,
    build_json_tool_example,
    format_tool_description_simple,
)
from loguru import logger
import json


############################################################################################################
class McpState(TypedDict, total=False):
    """
    MCP å¢å¼ºçš„çŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯å’Œ MCP å®¢æˆ·ç«¯ç›¸å…³ä¿¡æ¯
    """

    messages: Annotated[List[BaseMessage], add_messages]
    llm: ChatDeepSeek  # DeepSeek LLMå®ä¾‹ï¼Œæ•´ä¸ªgraphæµç¨‹å…±äº«ï¼ˆå¿…éœ€ï¼‰
    mcp_client: McpClient  # MCP å®¢æˆ·ç«¯ï¼ˆå¿…éœ€ï¼‰
    available_tools: List[McpToolInfo]  # å¯ç”¨çš„ MCP å·¥å…·
    tool_outputs: List[Dict[str, Any]]  # å·¥å…·æ‰§è¡Œç»“æœ

    # å·¥ä½œæµç¨‹å­—æ®µ
    first_llm_response: AIMessage  # ç¬¬ä¸€æ¬¡æ¨ç†ç»“æœï¼ˆå†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼‰
    parsed_tool_calls: List[Dict[str, Any]]  # è§£æå‡ºçš„å·¥å…·è°ƒç”¨
    needs_tool_execution: bool  # æ˜¯å¦éœ€è¦æ‰§è¡Œå·¥å…·

    # æœ€ç»ˆç»“æœ
    final_response: Optional[BaseMessage]  # æœ€ç»ˆå“åº”ï¼ˆæ¥è‡ªäºŒæ¬¡æ¨ç†æˆ–ç¬¬ä¸€æ¬¡æ¨ç†ï¼‰


############################################################################################################
def _build_tool_instruction_prompt(available_tools: List[McpToolInfo]) -> str:
    """
    æ„å»ºç³»ç»Ÿæç¤ºï¼Œä»…æ”¯æŒJSONæ ¼å¼å·¥å…·è°ƒç”¨

    Args:
        available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨

    Returns:
        str: æ„å»ºå¥½çš„ç³»ç»Ÿæç¤º
    """
    # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·ï¼Œæ²¡æœ‰å·¥å…·å°±ç›´æ¥è¿”å›ç®€å•æç¤º
    if not available_tools:
        return "âš ï¸ å½“å‰æ²¡æœ‰å¯ç”¨å·¥å…·ï¼Œè¯·ä»…ä½¿ç”¨ä½ çš„çŸ¥è¯†å›ç­”é—®é¢˜ã€‚"

    # æœ‰å·¥å…·æ—¶ï¼Œæ‰æ„å»ºå®Œæ•´çš„å·¥å…·è°ƒç”¨è¯´æ˜
    tool_instruction_prompt = """å½“ä½ éœ€è¦è·å–å®æ—¶ä¿¡æ¯æˆ–æ‰§è¡Œç‰¹å®šæ“ä½œæ—¶ï¼Œå¯ä»¥è°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚

## å·¥å…·è°ƒç”¨æ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è°ƒç”¨å·¥å…·ï¼ˆæ”¯æŒåŒæ—¶è°ƒç”¨å¤šä¸ªï¼‰ï¼š

```json
{
  "tool_call": {
    "name": "å·¥å…·åç§°1",
    "arguments": {
      "å‚æ•°å": "å‚æ•°å€¼1"
    }
  }
}

{
  "tool_call": {
    "name": "å·¥å…·åç§°2",
    "arguments": {
      "å‚æ•°å": "å‚æ•°å€¼2"
    }
  }
}
```

## ä½¿ç”¨æŒ‡å—

- å½“ä»»åŠ¡æ˜ç¡®è¦æ±‚ä½ è°ƒç”¨å·¥å…·æ—¶ï¼Œä½ å¿…é¡»è°ƒç”¨ç›¸åº”çš„å·¥å…·

**å·¥å…·è°ƒç”¨æµç¨‹**ï¼š
1. åˆ†æä»»åŠ¡éœ€æ±‚ï¼Œç¡®å®šéœ€è¦è°ƒç”¨å“ªäº›å·¥å…·
2. æŒ‰ç…§JSONæ ¼å¼è°ƒç”¨å·¥å…·ï¼ˆå¯åŒæ—¶è°ƒç”¨å¤šä¸ªï¼‰

**ç¦æ­¢è¡Œä¸º**ï¼š
- âŒ ä¸è¦åœ¨æœªè°ƒç”¨å·¥å…·çš„æƒ…å†µä¸‹å‡è®¾æˆ–æ¨æµ‹å·¥å…·æ‰§è¡Œç»“æœ"""

    # æ„å»ºå·¥å…·æè¿° - ç®€åŒ–ç‰ˆæœ¬ï¼Œç»Ÿä¸€ä½¿ç”¨çº¿æ€§å±•ç¤º
    tool_instruction_prompt += "\n\n## å¯ç”¨å·¥å…·"

    # ç›´æ¥åˆ—è¡¨å±•ç¤ºæ‰€æœ‰å·¥å…·ï¼Œæ— éœ€åˆ†ç±»
    for tool in available_tools:
        tool_desc = format_tool_description_simple(tool)
        tool_instruction_prompt += f"\n{tool_desc}"

    # æ·»åŠ å·¥å…·è°ƒç”¨ç¤ºä¾‹
    example_tool = available_tools[0]
    tool_instruction_prompt += f"\n\n## è°ƒç”¨ç¤ºä¾‹\n\n"
    tool_instruction_prompt += build_json_tool_example(example_tool)

    return tool_instruction_prompt


############################################################################################################
async def _preprocess_node(state: McpState) -> McpState:
    """
    é¢„å¤„ç†èŠ‚ç‚¹ï¼šå‡†å¤‡ç³»ç»Ÿæç¤ºå’Œå¢å¼ºæ¶ˆæ¯

    å…³é”®èŒè´£ï¼šç›´æ¥ä¿®æ”¹ messages ä¸Šä¸‹æ–‡ï¼Œæ³¨å…¥å·¥å…·è¯´æ˜
    åç»­èŠ‚ç‚¹åªèƒ½è¯»å– messagesï¼Œä¸èƒ½å†æ·»åŠ ä»»ä½•å†…å®¹

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        McpState: æ›´æ–°åçš„çŠ¶æ€
    """
    messages = state["messages"]
    available_tools = state.get("available_tools", [])

    # æ„å»ºç³»ç»Ÿæç¤º
    tool_instruction_prompt = _build_tool_instruction_prompt(available_tools)
    logger.debug(f"ğŸ› ï¸ å·¥å…·æŒ‡ä»¤æç¤º:\n{tool_instruction_prompt}")

    # æ™ºèƒ½æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼šç›´æ¥ä¿®æ”¹ messages
    if messages and isinstance(messages[0], SystemMessage):
        # å·²ç»æœ‰ç³»ç»Ÿæ¶ˆæ¯åœ¨å¼€å¤´ï¼Œè¿½åŠ æ–°çš„å·¥å…·è¯´æ˜
        messages.insert(1, SystemMessage(content=tool_instruction_prompt))
    else:
        # æ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ’å…¥é»˜è®¤è§’è‰²è®¾å®šå’Œå·¥å…·è¯´æ˜åˆ°å¼€å¤´
        default_role_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå…·æœ‰ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ã€‚\n\n" + tool_instruction_prompt
        )
        messages.insert(0, SystemMessage(content=default_role_prompt))

        # èµ°åˆ°è¿™é‡ŒåŸºæœ¬å°±æ˜¯é”™äº†ï¼Œè­¦å‘Šä¸‹ï¼Œå› ä¸ºä¼šå½±å“è§’è‰²è®¾å®šï¼
        logger.warning(
            "âš ï¸ ç³»ç»Ÿæ¶ˆæ¯ç¼ºå¤±ï¼Œå·²è‡ªåŠ¨æ·»åŠ é»˜è®¤è§’è‰²è®¾å®šå’Œå·¥å…·è¯´æ˜ï¼Œèµ°åˆ°è¿™é‡ŒåŸºæœ¬å°±æ˜¯é”™äº†ï¼Œè­¦å‘Šä¸‹ï¼Œå› ä¸ºä¼šå½±å“è§’è‰²è®¾å®šï¼"
        )

    # messages å·²ç›´æ¥ä¿®æ”¹ï¼Œé€šè¿‡è¿”å›è®© LangGraph æ„ŸçŸ¥å˜åŒ–
    result: McpState = {
        "messages": messages,
        "llm": state["llm"],
        "mcp_client": state["mcp_client"],
        "available_tools": available_tools,
        "tool_outputs": state.get("tool_outputs", []),
    }
    return result


############################################################################################################
async def _llm_invoke_node(state: McpState) -> McpState:
    """
    LLMè°ƒç”¨èŠ‚ç‚¹ï¼šç¬¬ä¸€æ¬¡æ¨ç†ï¼Œå†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·

    çº¦æŸï¼š
    - æ­£å¸¸æ—¶ï¼šè®¾ç½® first_llm_response å¹¶åŠ å…¥ messages
    - å¼‚å¸¸æ—¶ï¼šè®©å¼‚å¸¸å‘ä¸Šä¼ æ’­åˆ° execute_mcp_workflowï¼Œfinal_response ä¿æŒ None

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        McpState: æ›´æ–°åçš„çŠ¶æ€
    """
    llm = state["llm"]
    messages = state["messages"]

    # è°ƒç”¨ LLMï¼ˆå¦‚æœå¼‚å¸¸ï¼Œç›´æ¥å‘ä¸Šä¼ æ’­ï¼‰
    response = llm.invoke(messages)
    assert isinstance(response, AIMessage), "LLM è¿”å›çš„å“åº”å¿…é¡»æ˜¯ AIMessage ç±»å‹"

    return {
        "messages": [response],  # åŠ å…¥ messagesï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯
        "first_llm_response": response,  # ä¿å­˜å¼•ç”¨ä¾›åç»­èŠ‚ç‚¹ä½¿ç”¨
    }


############################################################################################################
async def _tool_parse_node(state: McpState) -> McpState:
    """
    å·¥å…·è§£æèŠ‚ç‚¹ï¼šè§£æLLMå“åº”ä¸­çš„å·¥å…·è°ƒç”¨

    çº¦æŸï¼š
    - first_llm_response å¿…é¡»å­˜åœ¨ï¼ˆä» llm_invoke_node ä¼ æ¥ï¼‰
    - è§£æå¤±è´¥æ—¶å¼‚å¸¸å‘ä¸Šä¼ æ’­

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        McpState: æ›´æ–°åçš„çŠ¶æ€ï¼ˆä»…åŒ…å« parsed_tool_calls å’Œ needs_tool_executionï¼‰
    """
    first_llm_response = state.get("first_llm_response")
    assert first_llm_response is not None, "first_llm_response å¿…é¡»å­˜åœ¨"

    available_tools = state.get("available_tools", [])
    parsed_tool_calls = []

    # åªæœ‰åœ¨æœ‰å¯ç”¨å·¥å…·æ—¶æ‰è§£æ
    if available_tools:
        response_content = str(first_llm_response.content or "")

        # ä½¿ç”¨å¢å¼ºçš„å·¥å…·è°ƒç”¨è§£æå™¨ï¼ˆå¦‚æœè§£æå¤±è´¥ï¼Œè®©å¼‚å¸¸å‘ä¸Šä¼ æ’­ï¼‰
        parser = ToolCallParser(available_tools)
        parsed_tool_calls = parser.parse_tool_calls(response_content)

        logger.info(f"ğŸ“‹ è§£æåˆ° {len(parsed_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
        for call in parsed_tool_calls:
            logger.debug(f"   - {call['name']}: {call['args']}")

    # åªè¿”å›æ”¹å˜çš„å­—æ®µï¼ŒLangGraph è‡ªåŠ¨ç»§æ‰¿å…¶ä»–å­—æ®µ
    return {
        "parsed_tool_calls": parsed_tool_calls,
        "needs_tool_execution": len(parsed_tool_calls) > 0,
    }


############################################################################################################
async def _tool_execution_node(state: McpState) -> McpState:
    """
    å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ï¼šå¹¶å‘æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œè¿”å›æ‰§è¡Œç»“æœ

    æ ¸å¿ƒèŒè´£ï¼šæ”¹å˜ tool_outputs å­—æ®µ
    çº¦æŸï¼š
    - asyncio.gather(return_exceptions=True) å·²å¤„ç†å•ä¸ªå·¥å…·å¼‚å¸¸
    - å¼‚å¸¸å‘ä¸Šä¼ æ’­åˆ° execute_mcp_workflow

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        McpState: æ›´æ–°åçš„çŠ¶æ€ï¼ˆä»…åŒ…å« tool_outputsï¼‰
    """
    parsed_tool_calls = state.get("parsed_tool_calls", [])
    mcp_client = state["mcp_client"]

    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›ç©ºç»“æœ
    if not parsed_tool_calls:
        return {"tool_outputs": []}

    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å·¥å…·
    logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œ {len(parsed_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")

    tasks = [
        execute_mcp_tool(
            call["name"],
            call["args"],
            mcp_client,
            timeout=30.0,
            max_retries=2,
        )
        for call in parsed_tool_calls
    ]

    # asyncio.gather å·²ç»å¤„ç†å¼‚å¸¸ (return_exceptions=True)
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # æ„å»º tool_outputs
    tool_outputs = []
    for call, result in zip(parsed_tool_calls, results):
        if isinstance(result, Exception):
            logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {call['name']}, é”™è¯¯: {result}")
            tool_outputs.append(
                {
                    "tool": call["name"],
                    "args": call["args"],
                    "result": f"æ‰§è¡Œå¤±è´¥: {str(result)}",
                    "success": False,
                    "execution_time": 0.0,
                }
            )
        elif isinstance(result, tuple) and len(result) == 3:
            success, task_result, exec_time = result
            tool_outputs.append(
                {
                    "tool": call["name"],
                    "args": call["args"],
                    "result": task_result,
                    "success": success,
                    "execution_time": exec_time,
                }
            )
        else:
            # æ„å¤–çš„ç»“æœç±»å‹ï¼Œè®°å½•é”™è¯¯
            logger.error(f"å·¥å…·è¿”å›æ„å¤–ç»“æœç±»å‹: {call['name']}, ç»“æœ: {result}")
            tool_outputs.append(
                {
                    "tool": call["name"],
                    "args": call["args"],
                    "result": f"æ„å¤–ç»“æœç±»å‹: {type(result)}",
                    "success": False,
                    "execution_time": 0.0,
                }
            )

    # ç»Ÿè®¡æ—¥å¿—
    successful = sum(1 for o in tool_outputs if o["success"])
    total_time = sum(o["execution_time"] for o in tool_outputs)
    logger.info(
        f"âœ… å·¥å…·æ‰§è¡Œå®Œæˆ: {successful}/{len(tool_outputs)} æˆåŠŸ, æ€»è€—æ—¶: {total_time:.2f}s"
    )
    logger.debug(
        f"å·¥å…·æ‰§è¡Œè®°å½•: {json.dumps(tool_outputs, indent=2, ensure_ascii=False)}"
    )

    # åªè¿”å›æ”¹å˜çš„å­—æ®µ
    return {"tool_outputs": tool_outputs}


############################################################################################################
def _build_tool_context(tool_outputs: List[Dict[str, Any]]) -> str:
    """
    æ„å»ºå·¥å…·æ‰§è¡Œç»“æœçš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²

    Args:
        tool_outputs: å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨

    Returns:
        str: æ ¼å¼åŒ–çš„å·¥å…·ç»“æœä¸Šä¸‹æ–‡
    """
    tool_context_parts = []
    for i, output in enumerate(tool_outputs, 1):
        tool_name = output.get("tool", "æœªçŸ¥å·¥å…·")
        success = output.get("success", False)
        result_data = output.get("result", "æ— ç»“æœ")
        exec_time = output.get("execution_time", 0.0)

        status = "æˆåŠŸ" if success else "å¤±è´¥"
        tool_context_parts.append(
            f"å·¥å…·{i}: {tool_name} (æ‰§è¡Œ{status}, è€—æ—¶{exec_time:.2f}s)\n"
            f"ç»“æœ: {result_data}"
        )

    return "\n\n".join(tool_context_parts)


############################################################################################################
async def _llm_re_invoke_node(state: McpState) -> McpState:
    """
    äºŒæ¬¡æ¨ç†èŠ‚ç‚¹ï¼šåŸºäºå·¥å…·æ‰§è¡Œç»“æœé‡æ–°è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ†æ

    è¿™æ˜¯æ–°æ¶æ„çš„æ ¸å¿ƒèŠ‚ç‚¹ï¼Œè§£å†³äº†å·¥å…·ç»“æœåªæ˜¯ç®€å•æ‹¼æ¥çš„é—®é¢˜ã€‚
    è®©AIèƒ½å¤ŸåŸºäºå·¥å…·ç»“æœè¿›è¡Œæ·±åº¦åˆ†æå’Œä¸ªæ€§åŒ–å›ç­”ã€‚

    çº¦æŸï¼š
    - åªè¯»å– messagesï¼Œä¸æ·»åŠ ä»»ä½•å†…å®¹
    - å¼‚å¸¸å‘ä¸Šä¼ æ’­åˆ° execute_mcp_workflow

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        McpState: æ›´æ–°åçš„çŠ¶æ€
    """
    tool_outputs = state.get("tool_outputs", [])

    # æ²¡æœ‰å·¥å…·è¾“å‡ºæ—¶ï¼Œç›´æ¥è¿”å›ç¬¬ä¸€æ¬¡æ¨ç†ç»“æœ
    if not tool_outputs:
        first_llm_response = state.get("first_llm_response")
        assert first_llm_response is not None, "first_llm_response å¿…é¡»å­˜åœ¨"
        return {"final_response": first_llm_response}

    # æœ‰å·¥å…·è¾“å‡ºï¼Œè¿›è¡ŒäºŒæ¬¡æ¨ç†
    llm = state["llm"]

    # æ„å»ºå·¥å…·ç»“æœä¸Šä¸‹æ–‡
    tool_context = _build_tool_context(tool_outputs)

    # æ‹†åˆ†æ¶ˆæ¯ï¼šAIMessage(å·¥å…·ç»“æœ) + HumanMessage(çº¦æŸå’Œè¦æ±‚)
    tool_result_message = AIMessage(content=tool_context)

    user_feedback_message = HumanMessage(
        content="""åŸºäºä¸Šè¿°å·¥å…·æ‰§è¡Œç»“æœï¼Œè¯·ç›´æ¥å“åº”ç”¨æˆ·è¾“å…¥ã€‚

---

## âš ï¸ çº¦æŸæ¡ä»¶

- **ç¦æ­¢å†æ¬¡è°ƒç”¨å·¥å…·** - æ‰€æœ‰å·¥å…·å·²æ‰§è¡Œå®Œæˆ
- **ç¦æ­¢è¾“å‡ºå·¥å…·è°ƒç”¨æ ¼å¼** - ä¸è¦ç”Ÿæˆ {"tool_call": ...} è¿™æ ·çš„JSONç»“æ„

## âœ… å“åº”è¦æ±‚

1. **å†…å®¹**: åŸºäºå·¥å…·ç»“æœç›´æ¥å“åº”ç”¨æˆ·è¾“å…¥ï¼Œä¿æŒä½ çš„è§’è‰²è®¾å®šå’Œè¯­è¨€é£æ ¼
2. **æ ¼å¼**: å¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚ç‰¹å®šè¾“å‡ºæ ¼å¼(JSON/Markdown/è¡¨æ ¼ç­‰)ï¼Œä¸¥æ ¼éµå®ˆ
3. **é£æ ¼**: è‡ªç„¶èåˆå·¥å…·ç»“æœè¿›è¡Œå›åº”ï¼Œæ— éœ€è§£é‡Šå·¥å…·è°ƒç”¨è¿‡ç¨‹

ğŸ’¡ **æç¤º**: ç”¨æˆ·è¾“å…¥å¯èƒ½æ˜¯é—®é¢˜ã€æŒ‡ä»¤ã€å¯¹è¯æˆ–è¡ŒåŠ¨æè¿°ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡çµæ´»å“åº”ã€‚"""
    )

    # ç›´æ¥ä¿®æ”¹ state["messages"]ï¼Œæ·»åŠ å·¥å…·ç»“æœå’Œç”¨æˆ·åé¦ˆ
    messages = state["messages"]
    messages.append(tool_result_message)
    messages.append(user_feedback_message)

    # äºŒæ¬¡è°ƒç”¨ LLMï¼ˆå¼‚å¸¸å‘ä¸Šä¼ æ’­ï¼‰
    logger.info("ğŸ”„ å¼€å§‹äºŒæ¬¡æ¨ç†ï¼ŒåŸºäºå·¥å…·ç»“æœç”Ÿæˆæ™ºèƒ½å›ç­”...")
    re_invoke_response = llm.invoke(messages)
    logger.info("âœ… äºŒæ¬¡æ¨ç†å®Œæˆ")

    # å°†æœ€ç»ˆå“åº”ä¹ŸåŠ å…¥ messagesï¼Œä¿æŒå®Œæ•´é“¾è·¯
    messages.append(re_invoke_response)

    return {"final_response": re_invoke_response}


############################################################################################################
def _should_execute_tools(state: McpState) -> str:
    """
    æ¡ä»¶è·¯ç”±ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œå·¥å…·

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        str: ä¸‹ä¸€ä¸ªèŠ‚ç‚¹åç§°
    """
    needs_tool_execution = state.get("needs_tool_execution", False)
    return "tool_execution" if needs_tool_execution else "response_synthesis"


############################################################################################################
def create_mcp_workflow() -> CompiledStateGraph[McpState, Any, McpState, McpState]:
    """
    åˆ›å»ºå¸¦ MCP æ”¯æŒçš„ç¼–è¯‘çŠ¶æ€å›¾ï¼ˆå¤šèŠ‚ç‚¹æ¶æ„ï¼‰

    å·¥ä½œæµæ¶æ„ï¼š
    preprocess â†’ llm_invoke â†’ tool_parse â†’ [æ¡ä»¶åˆ¤æ–­]
                                             â†“ (éœ€è¦å·¥å…·)
                                        tool_execution â†’ llm_re_invoke (ç»“æŸ)
                                             â†“ (ä¸éœ€è¦å·¥å…·)
                                        llm_invoke (ç»“æŸ)

    Returns:
        CompiledStateGraph: ç¼–è¯‘åçš„çŠ¶æ€å›¾
    """

    # æ„å»ºå¤šèŠ‚ç‚¹çŠ¶æ€å›¾
    graph_builder = StateGraph(McpState)

    # æ·»åŠ å„ä¸ªèŠ‚ç‚¹
    graph_builder.add_node("preprocess", _preprocess_node)
    graph_builder.add_node("llm_invoke", _llm_invoke_node)
    graph_builder.add_node("tool_parse", _tool_parse_node)
    graph_builder.add_node("tool_execution", _tool_execution_node)
    graph_builder.add_node("llm_re_invoke", _llm_re_invoke_node)

    # è®¾ç½®æµç¨‹è·¯å¾„
    graph_builder.set_entry_point("preprocess")
    graph_builder.add_edge("preprocess", "llm_invoke")
    graph_builder.add_edge("llm_invoke", "tool_parse")

    # æ¡ä»¶è·¯ç”±ï¼šå·¥å…·è§£æååˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œå·¥å…·
    graph_builder.add_conditional_edges(
        "tool_parse",
        _should_execute_tools,
        {
            "tool_execution": "tool_execution",  # éœ€è¦å·¥å…· â†’ å·¥å…·æ‰§è¡Œ
            "llm_invoke": "__end__",  # ä¸éœ€è¦å·¥å…· â†’ ç›´æ¥ç»“æŸ
        },
    )

    # å·¥å…·æ‰§è¡Œåè¿›å…¥äºŒæ¬¡æ¨ç†ï¼Œç„¶åç»“æŸ
    graph_builder.add_edge("tool_execution", "llm_re_invoke")
    graph_builder.add_edge("llm_re_invoke", "__end__")

    return graph_builder.compile()  # type: ignore[return-value]


############################################################################################################
async def execute_mcp_workflow(
    work_flow: CompiledStateGraph[McpState, Any, McpState, McpState],
    context: List[BaseMessage],
    request: HumanMessage,
    llm: ChatDeepSeek,
    mcp_client: McpClient,
) -> List[BaseMessage]:
    """æ‰§è¡ŒMCPå·¥ä½œæµå¹¶è¿”å›æ‰€æœ‰å“åº”æ¶ˆæ¯

    å°†èŠå¤©å†å²å’Œç”¨æˆ·è¾“å…¥åˆå¹¶åï¼Œé€šè¿‡ç¼–è¯‘å¥½çš„çŠ¶æ€å›¾è¿›è¡ŒMCPå·¥å…·è°ƒç”¨å¤„ç†ï¼Œ
    æ”¶é›†å¹¶è¿”å›æ‰€æœ‰ç”Ÿæˆçš„æ¶ˆæ¯ã€‚

    Args:
        work_flow: å·²ç¼–è¯‘çš„ LangGraph çŠ¶æ€å›¾
        context: å†å²æ¶ˆæ¯åˆ—è¡¨
        request: ç”¨æˆ·å½“å‰è¾“å…¥çš„æ¶ˆæ¯
        llm: ChatDeepSeek LLM å®ä¾‹
        mcp_client: MCP å®¢æˆ·ç«¯å®ä¾‹

    Returns:
        åŒ…å«æ‰€æœ‰ç”Ÿæˆæ¶ˆæ¯çš„åˆ—è¡¨
    """
    ret: List[BaseMessage] = []

    # åœ¨å‡½æ•°å†…éƒ¨è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
    available_tools = await mcp_client.list_tools()
    if available_tools is None:
        available_tools = []

    # æ„é€  McpStateï¼ˆcontext + [request] åˆ›å»ºæ–°åˆ—è¡¨ï¼Œé¿å…ä¿®æ”¹ä¼ å…¥å‚æ•°ï¼‰
    workflow_state_context: McpState = {
        "messages": context + [request],
        "llm": llm,
        "mcp_client": mcp_client,
        "available_tools": available_tools,
        "tool_outputs": [],
    }

    try:

        # æœ€ç»ˆçŠ¶æ€
        final_state: Optional[McpState] = None

        # æµå¼å¤„ç†æ‰€æœ‰èŠ‚ç‚¹çš„æ›´æ–°
        async for event in work_flow.astream(workflow_state_context):
            for node_name, value in event.items():
                # æŒç»­æ›´æ–°çŠ¶æ€ï¼Œæœ€åä¸€ä¸ªå°±æ˜¯æœ€ç»ˆçŠ¶æ€
                final_state = value

        # âœ… å…³é”®æ”¹è¿›ï¼šä»æœ€ç»ˆçŠ¶æ€çš„ final_response å­—æ®µè·å–ç»“æœï¼Œä¸ä¾èµ–èŠ‚ç‚¹åç§°
        if final_state:
            final_response = final_state.get("final_response")
            if final_response:
                logger.info("âœ… ä»çŠ¶æ€çš„ final_response å­—æ®µè·å–æœ€ç»ˆå“åº”")
                ret.append(final_response)
            else:
                logger.error(
                    "âŒ final_response ä¸å­˜åœ¨ï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿï¼ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½åº”è¯¥è®¾ç½® final_responseï¼‰"
                )
        else:
            logger.error("âŒ æœªè·å–åˆ°æœ€ç»ˆçŠ¶æ€")

    except Exception as e:
        logger.error(f"Stream processing error: {e}")

    return ret


############################################################################################################
