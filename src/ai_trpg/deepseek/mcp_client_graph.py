"""
MCP Client Graph - åŸºäº LangGraph çš„ MCP å·¥å…·è°ƒç”¨å·¥ä½œæµ

## å·¥ä½œæµæ¶æ„
preprocess â†’ llm_invoke â†’ tool_parse â†’ [æ¡ä»¶è·¯ç”±]
                                          â†“ (éœ€è¦å·¥å…·)
                                    tool_execution â†’ llm_re_invoke â†’ END
                                          â†“ (æ— éœ€å·¥å…·)
                                        END

## æ ¸å¿ƒè®¾è®¡ï¼šäºŒæ¬¡æ¨ç†ï¼ˆRe-invokeï¼‰æ¨¡å¼
è®© AI åŸºäºå·¥å…·ç»“æœè¿›è¡Œæ·±åº¦åˆ†æï¼Œè€Œä¸æ˜¯ç®€å•æ‹¼æ¥ã€‚

## messages ä¸Šä¸‹æ–‡æ¼”å˜
1. **åˆå§‹åŒ–**: `context + [request]`
2. **preprocess**: æ’å…¥ `SystemMessage(å·¥å…·è¯´æ˜)`
3. **llm_invoke**: è¿½åŠ  `AIMessage(first_llm_response)`
4. **tool_parse**: è§£æå·¥å…·è°ƒç”¨ï¼ˆä¸ä¿®æ”¹ messagesï¼‰
5. **tool_execution**: å¹¶å‘æ‰§è¡Œå·¥å…·ï¼ˆä¸ä¿®æ”¹ messagesï¼‰
6. **llm_re_invoke**: è¿½åŠ  `AIMessage(å·¥å…·ç»“æœ)` + `HumanMessage(äºŒæ¬¡æ¨ç†æŒ‡ä»¤)` + `AIMessage(re_invoke_response)`

## å…³é”®å­—æ®µ
- `first_llm_response`: ç¬¬ä¸€æ¬¡æ¨ç†ç»“æœï¼ˆç”¨äºæå–ï¼‰
- `re_invoke_response`: äºŒæ¬¡æ¨ç†ç»“æœï¼ˆç”¨äºæå–ï¼‰
- `messages`: å®Œæ•´ä¸Šä¸‹æ–‡é“¾è·¯ï¼ˆå…¨å±€å”¯ä¸€çœŸç›¸æºï¼‰

## è¿”å›é€»è¾‘
- æœ‰å·¥å…·æ‰§è¡Œ â†’ è¿”å› `re_invoke_response`
- æ— å·¥å…·æ‰§è¡Œ â†’ è¿”å› `first_llm_response`

## âš ï¸ LangGraph çŠ¶æ€åˆå¹¶æœºåˆ¶ï¼ˆé‡è¦ï¼ï¼‰

**å…³é”®è§„åˆ™**ï¼š
1. âœ… å¸¦ `Annotated` ä¿®é¥°ç¬¦çš„å­—æ®µï¼ˆå¦‚ `messages`ï¼‰ä¼šè‡ªåŠ¨ç´¯ç§¯åˆå¹¶
2. âŒ æ™®é€šå­—æ®µï¼ˆå¦‚ `llm`, `mcp_client`ï¼‰**å®Œå…¨æ›¿æ¢ï¼Œä¸åˆå¹¶**
3. ğŸš¨ **å¦‚æœèŠ‚ç‚¹è¿”å›å€¼ä¸­ç¼ºå°‘æŸä¸ªå­—æ®µï¼Œè¯¥å­—æ®µä¼šä»çŠ¶æ€ä¸­ä¸¢å¤±ï¼**

**æ­£ç¡®åšæ³•**ï¼š
```python
# âœ… èŠ‚ç‚¹å¿…é¡»è¿”å›æ‰€æœ‰éœ€è¦ä¿æŒçš„å­—æ®µ
return {
    "messages": state["messages"],      # ä¿æŒ
    "llm": state["llm"],                # ä¿æŒ
    "mcp_client": state["mcp_client"],  # ä¿æŒ
    "new_field": new_value,             # æ–°å¢/æ›´æ–°
}

# âŒ é”™è¯¯ï¼šåªè¿”å›æ–°å­—æ®µä¼šå¯¼è‡´å…¶ä»–å­—æ®µä¸¢å¤±
return {
    "new_field": new_value,  # å…¶ä»–å­—æ®µä¼šä»çŠ¶æ€ä¸­æ¶ˆå¤±ï¼
}
```

**å¯¹æ¯”å…¶ä»– Graph**ï¼š
- `chat_graph.py` å’Œ `rag_graph.py` çš„èŠ‚ç‚¹éƒ½æ­£ç¡®ä¿æŒäº†æ‰€æœ‰å¿…è¦å­—æ®µ
- æœ¬æ–‡ä»¶ä¹‹å‰çš„å®ç°å­˜åœ¨å­—æ®µä¸¢å¤±é—®é¢˜ï¼Œå·²ä¿®å¤
"""

from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

import asyncio
from typing import Annotated, Any, Dict, Final, List, Optional
from langchain.schema import AIMessage, SystemMessage, HumanMessage
from langchain_core.messages import BaseMessage
from langchain_deepseek import ChatDeepSeek
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.graph.state import CompiledStateGraph
from typing_extensions import TypedDict
from ..mcp import (
    McpClient,
    McpToolInfo,
    ToolCallParser,
    execute_mcp_tool,
    build_json_tool_example,
    format_tool_description_simple,
)
from loguru import logger
import json


############################################################################################################
# é»˜è®¤çš„äºŒæ¬¡æ¨ç†æŒ‡ä»¤æ¨¡æ¿ï¼ˆå¸¸é‡ï¼‰
DEFAULT_RE_INVOKE_INSTRUCTION: Final[
    str
] = """# åŸºäºä¸Šè¿°å·¥å…·æ‰§è¡Œç»“æœï¼Œå“åº”ç”¨æˆ·è¾“å…¥!

## âš ï¸ çº¦æŸæ¡ä»¶

- **ç¦æ­¢å†æ¬¡è°ƒç”¨å·¥å…·** - æ‰€æœ‰å·¥å…·å·²æ‰§è¡Œå®Œæˆ
- **ç¦æ­¢è¾“å‡ºå·¥å…·è°ƒç”¨æ ¼å¼** - ä¸è¦ç”Ÿæˆ {"tool_call": ...} è¿™æ ·çš„JSONç»“æ„

## âœ… å“åº”è¦æ±‚

1. **å†…å®¹**: åŸºäºå·¥å…·ç»“æœç›´æ¥å“åº”ç”¨æˆ·è¾“å…¥ï¼Œä¿æŒä½ çš„è§’è‰²è®¾å®šå’Œè¯­è¨€é£æ ¼
2. **æ ¼å¼**: å¦‚æœç”¨æˆ·åœ¨æœ€è¿‘ä¸€æ¬¡çš„è¯·æ±‚ä¸­æ˜ç¡®è¦æ±‚ç‰¹å®šè¾“å‡ºæ ¼å¼(JSON/Markdown/è¡¨æ ¼ç­‰)ï¼Œä¸¥æ ¼éµå®ˆ
3. **é£æ ¼**: è‡ªç„¶èåˆå·¥å…·ç»“æœè¿›è¡Œå›åº”ï¼Œæ— éœ€è§£é‡Šå·¥å…·è°ƒç”¨è¿‡ç¨‹

ğŸ’¡ **æç¤º**: ç”¨æˆ·è¾“å…¥å¯èƒ½æ˜¯é—®é¢˜ã€æŒ‡ä»¤ã€å¯¹è¯æˆ–è¡ŒåŠ¨æè¿°ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡çµæ´»å“åº”ã€‚"""


############################################################################################################
# å·¥å…·è°ƒç”¨æŒ‡ä»¤æ¨¡æ¿ï¼ˆå¸¸é‡ï¼‰
TOOL_CALL_INSTRUCTION: Final[
    str
] = """å½“ä½ éœ€è¦è·å–å®æ—¶ä¿¡æ¯æˆ–æ‰§è¡Œç‰¹å®šæ“ä½œæ—¶ï¼Œå¯ä»¥è°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚

## å·¥å…·è°ƒç”¨æ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è°ƒç”¨å·¥å…·ï¼ˆæ”¯æŒåŒæ—¶è°ƒç”¨å¤šä¸ªï¼‰ï¼š

```json
{
  "tool_call": {
    "name": "å·¥å…·åç§°1",
    "arguments": {
      "å‚æ•°å": "å‚æ•°å€¼1"
    }
  }
}

{
  "tool_call": {
    "name": "å·¥å…·åç§°2",
    "arguments": {
      "å‚æ•°å": "å‚æ•°å€¼2"
    }
  }
}
```

## ä½¿ç”¨æŒ‡å—

- å½“ä»»åŠ¡æ˜ç¡®è¦æ±‚ä½ è°ƒç”¨å·¥å…·æ—¶ï¼Œä½ å¿…é¡»è°ƒç”¨ç›¸åº”çš„å·¥å…·

**å·¥å…·è°ƒç”¨æµç¨‹**ï¼š
1. åˆ†æä»»åŠ¡éœ€æ±‚ï¼Œç¡®å®šéœ€è¦è°ƒç”¨å“ªäº›å·¥å…·
2. æŒ‰ç…§JSONæ ¼å¼è°ƒç”¨å·¥å…·ï¼ˆå¯åŒæ—¶è°ƒç”¨å¤šä¸ªï¼‰

**ç¦æ­¢è¡Œä¸º**ï¼š
- âŒ ä¸è¦åœ¨æœªè°ƒç”¨å·¥å…·çš„æƒ…å†µä¸‹å‡è®¾æˆ–æ¨æµ‹å·¥å…·æ‰§è¡Œç»“æœ"""


############################################################################################################
class McpState(TypedDict, total=False):
    """
    MCP å¢å¼ºçš„çŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯å’Œ MCP å®¢æˆ·ç«¯ç›¸å…³ä¿¡æ¯
    """

    messages: Annotated[List[BaseMessage], add_messages]
    llm: ChatDeepSeek  # DeepSeek LLMå®ä¾‹ï¼Œæ•´ä¸ªgraphæµç¨‹å…±äº«ï¼ˆå¿…éœ€ï¼‰
    mcp_client: McpClient  # MCP å®¢æˆ·ç«¯ï¼ˆå¿…éœ€ï¼‰
    available_tools: List[McpToolInfo]  # å¯ç”¨çš„ MCP å·¥å…·
    tool_outputs: List[Dict[str, Any]]  # å·¥å…·æ‰§è¡Œç»“æœ

    # å·¥ä½œæµç¨‹å­—æ®µ
    first_llm_response: AIMessage  # ç¬¬ä¸€æ¬¡æ¨ç†ç»“æœï¼ˆå†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼‰
    parsed_tool_calls: List[Dict[str, Any]]  # è§£æå‡ºçš„å·¥å…·è°ƒç”¨
    needs_tool_execution: bool  # æ˜¯å¦éœ€è¦æ‰§è¡Œå·¥å…·
    re_invoke_response: AIMessage  # äºŒæ¬¡æ¨ç†ç»“æœï¼ˆä»…åœ¨æ‰§è¡Œå·¥å…·åå­˜åœ¨ï¼‰
    re_invoke_instruction: Optional[HumanMessage]  # äºŒæ¬¡æ¨ç†æŒ‡ä»¤æ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰


############################################################################################################
def _build_tool_instruction_prompt(available_tools: List[McpToolInfo]) -> str:
    """
    æ„å»ºç³»ç»Ÿæç¤ºï¼Œä»…æ”¯æŒJSONæ ¼å¼å·¥å…·è°ƒç”¨

    Args:
        available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨

    Returns:
        str: æ„å»ºå¥½çš„ç³»ç»Ÿæç¤º
    """
    # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·ï¼Œæ²¡æœ‰å·¥å…·å°±ç›´æ¥è¿”å›ç®€å•æç¤º
    if not available_tools:
        return "âš ï¸ å½“å‰æ²¡æœ‰å¯ç”¨å·¥å…·ï¼Œè¯·ä»…ä½¿ç”¨ä½ çš„çŸ¥è¯†å›ç­”é—®é¢˜ã€‚"

    # ä½¿ç”¨å¸¸é‡æ¨¡æ¿ä½œä¸ºåŸºç¡€
    tool_instruction_prompt = str(TOOL_CALL_INSTRUCTION)

    # æ·»åŠ å¯ç”¨å·¥å…·åˆ—è¡¨
    tool_instruction_prompt += "\n\n## å¯ç”¨å·¥å…·"

    # ç›´æ¥åˆ—è¡¨å±•ç¤ºæ‰€æœ‰å·¥å…·ï¼Œæ— éœ€åˆ†ç±»
    for tool in available_tools:
        tool_desc = format_tool_description_simple(tool)
        tool_instruction_prompt += f"\n{tool_desc}"

    # æ·»åŠ å·¥å…·è°ƒç”¨ç¤ºä¾‹
    example_tool = available_tools[0]
    tool_instruction_prompt += "\n\n## è°ƒç”¨ç¤ºä¾‹\n\n"
    tool_instruction_prompt += build_json_tool_example(example_tool)

    return tool_instruction_prompt


############################################################################################################
async def _preprocess_node(state: McpState) -> McpState:
    """
    é¢„å¤„ç†èŠ‚ç‚¹ï¼šæ³¨å…¥å·¥å…·è¯´æ˜åˆ° messages

    messages å˜åŒ–ï¼šæ’å…¥ SystemMessage(å·¥å…·è¯´æ˜)

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        McpState: æ›´æ–°åçš„çŠ¶æ€
    """
    messages = state["messages"]
    available_tools = state.get("available_tools", [])

    # æ„å»ºç³»ç»Ÿæç¤º
    tool_instruction_prompt = _build_tool_instruction_prompt(available_tools)
    # logger.debug(f"ğŸ› ï¸ å·¥å…·æŒ‡ä»¤æç¤º:\n{tool_instruction_prompt}")

    # æ™ºèƒ½æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼šç›´æ¥ä¿®æ”¹ messages
    if messages and isinstance(messages[0], SystemMessage):
        # å·²ç»æœ‰ç³»ç»Ÿæ¶ˆæ¯åœ¨å¼€å¤´ï¼Œè¿½åŠ æ–°çš„å·¥å…·è¯´æ˜
        messages.insert(1, SystemMessage(content=tool_instruction_prompt))
    else:
        # æ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ’å…¥é»˜è®¤è§’è‰²è®¾å®šå’Œå·¥å…·è¯´æ˜åˆ°å¼€å¤´
        default_role_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå…·æœ‰ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ã€‚\n\n" + tool_instruction_prompt
        )
        messages.insert(0, SystemMessage(content=default_role_prompt))

        # èµ°åˆ°è¿™é‡ŒåŸºæœ¬å°±æ˜¯é”™äº†ï¼Œè­¦å‘Šä¸‹ï¼Œå› ä¸ºä¼šå½±å“è§’è‰²è®¾å®šï¼
        logger.warning(
            "âš ï¸ ç³»ç»Ÿæ¶ˆæ¯ç¼ºå¤±ï¼Œå·²è‡ªåŠ¨æ·»åŠ é»˜è®¤è§’è‰²è®¾å®šå’Œå·¥å…·è¯´æ˜ï¼Œèµ°åˆ°è¿™é‡ŒåŸºæœ¬å°±æ˜¯é”™äº†ï¼Œè­¦å‘Šä¸‹ï¼Œå› ä¸ºä¼šå½±å“è§’è‰²è®¾å®šï¼"
        )

    # âœ… å¿…é¡»ä¿æŒæ‰€æœ‰å¿…è¦çš„çŠ¶æ€å­—æ®µï¼
    result: McpState = {
        "messages": messages,
        "llm": state["llm"],
        "mcp_client": state["mcp_client"],
        "available_tools": available_tools,
        "tool_outputs": state.get("tool_outputs", []),
    }
    return result


############################################################################################################
async def _llm_invoke_node(state: McpState) -> McpState:
    """
    ç¬¬ä¸€æ¬¡æ¨ç†èŠ‚ç‚¹ï¼šå†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·

    messages å˜åŒ–ï¼šè¿½åŠ  AIMessage(first_llm_response)

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        McpState: åŒ…å« first_llm_response çš„çŠ¶æ€
    """
    llm = state["llm"]
    messages = state["messages"]

    # è°ƒç”¨ LLMï¼ˆå¦‚æœå¼‚å¸¸ï¼Œç›´æ¥å‘ä¸Šä¼ æ’­ï¼‰
    response = llm.invoke(messages)
    assert isinstance(response, AIMessage), "LLM è¿”å›çš„å“åº”å¿…é¡»æ˜¯ AIMessage ç±»å‹"

    # âœ… ä¿æŒæ‰€æœ‰å¿…è¦å­—æ®µ
    return {
        "messages": [response],  # add_messages ä¼šè‡ªåŠ¨åˆå¹¶
        "llm": llm,
        "mcp_client": state["mcp_client"],
        "available_tools": state.get("available_tools", []),
        "tool_outputs": state.get("tool_outputs", []),
        "first_llm_response": response,  # æ–°å¢å­—æ®µ
    }


############################################################################################################
async def _tool_parse_node(state: McpState) -> McpState:
    """
    å·¥å…·è§£æèŠ‚ç‚¹ï¼šè§£æ LLM å“åº”ä¸­çš„å·¥å…·è°ƒç”¨

    messages å˜åŒ–ï¼šæ— ï¼ˆåªè¯»å–ï¼‰

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        McpState: åŒ…å« parsed_tool_calls å’Œ needs_tool_execution
    """
    first_llm_response = state.get("first_llm_response")
    assert first_llm_response is not None, "first_llm_response å¿…é¡»å­˜åœ¨"

    available_tools = state.get("available_tools", [])
    parsed_tool_calls = []

    # åªæœ‰åœ¨æœ‰å¯ç”¨å·¥å…·æ—¶æ‰è§£æ
    if available_tools:
        response_content = str(first_llm_response.content or "")

        # ä½¿ç”¨å¢å¼ºçš„å·¥å…·è°ƒç”¨è§£æå™¨ï¼ˆå¦‚æœè§£æå¤±è´¥ï¼Œè®©å¼‚å¸¸å‘ä¸Šä¼ æ’­ï¼‰
        parser = ToolCallParser(available_tools)
        parsed_tool_calls = parser.parse_tool_calls(response_content)

        # logger.info(f"ğŸ“‹ è§£æåˆ° {len(parsed_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
        # for call in parsed_tool_calls:
        #     logger.debug(f"   - {call['name']}: {call['args']}")

    # âœ… ä¿æŒæ‰€æœ‰å¿…è¦å­—æ®µ
    return {
        "messages": state["messages"],
        "llm": state["llm"],
        "mcp_client": state["mcp_client"],
        "available_tools": available_tools,
        "tool_outputs": state.get("tool_outputs", []),
        "first_llm_response": first_llm_response,
        "parsed_tool_calls": parsed_tool_calls,  # æ–°å¢å­—æ®µ
        "needs_tool_execution": len(parsed_tool_calls) > 0,  # æ–°å¢å­—æ®µ
    }


############################################################################################################
async def _tool_execution_node(state: McpState) -> McpState:
    """
    å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ï¼šå¹¶å‘æ‰§è¡Œå·¥å…·è°ƒç”¨

    messages å˜åŒ–ï¼šæ— ï¼ˆåªè¯»å–ï¼‰

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        McpState: åŒ…å« tool_outputs
    """
    parsed_tool_calls = state.get("parsed_tool_calls", [])
    mcp_client = state["mcp_client"]

    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›ç©ºç»“æœï¼ˆä½†ä¿æŒæ‰€æœ‰å­—æ®µï¼‰
    if not parsed_tool_calls:
        return {
            "messages": state["messages"],
            "llm": state["llm"],
            "mcp_client": mcp_client,
            "available_tools": state.get("available_tools", []),
            "first_llm_response": state["first_llm_response"],
            "parsed_tool_calls": parsed_tool_calls,
            "needs_tool_execution": state.get("needs_tool_execution", False),
            "tool_outputs": [],
        }

    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å·¥å…·
    # logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œ {len(parsed_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")

    tasks = [
        execute_mcp_tool(
            call["name"],
            call["args"],
            mcp_client,
            timeout=30.0,
            max_retries=2,
        )
        for call in parsed_tool_calls
    ]

    # asyncio.gather å·²ç»å¤„ç†å¼‚å¸¸ (return_exceptions=True)
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # æ„å»º tool_outputs
    tool_outputs = []
    for call, result in zip(parsed_tool_calls, results):
        if isinstance(result, Exception):
            logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {call['name']}, é”™è¯¯: {result}")
            tool_outputs.append(
                {
                    "tool": call["name"],
                    "args": call["args"],
                    "result": f"æ‰§è¡Œå¤±è´¥: {str(result)}",
                    "success": False,
                    "execution_time": 0.0,
                }
            )
        elif isinstance(result, tuple) and len(result) == 3:
            success, task_result, exec_time = result
            tool_outputs.append(
                {
                    "tool": call["name"],
                    "args": call["args"],
                    "result": task_result,
                    "success": success,
                    "execution_time": exec_time,
                }
            )
        else:
            # æ„å¤–çš„ç»“æœç±»å‹ï¼Œè®°å½•é”™è¯¯
            logger.error(f"å·¥å…·è¿”å›æ„å¤–ç»“æœç±»å‹: {call['name']}, ç»“æœ: {result}")
            tool_outputs.append(
                {
                    "tool": call["name"],
                    "args": call["args"],
                    "result": f"æ„å¤–ç»“æœç±»å‹: {type(result)}",
                    "success": False,
                    "execution_time": 0.0,
                }
            )

    # ç»Ÿè®¡æ—¥å¿—
    successful = sum(1 for o in tool_outputs if o["success"])
    total_time = sum(o["execution_time"] for o in tool_outputs)
    logger.info(
        f"âœ… å·¥å…·æ‰§è¡Œå®Œæˆ: {successful}/{len(tool_outputs)} æˆåŠŸ, æ€»è€—æ—¶: {total_time:.2f}s"
    )
    logger.debug(
        f"å·¥å…·æ‰§è¡Œè®°å½•: {json.dumps(tool_outputs, indent=2, ensure_ascii=False)}"
    )

    # âœ… ä¿æŒæ‰€æœ‰å¿…è¦å­—æ®µ
    return {
        "messages": state["messages"],
        "llm": state["llm"],
        "mcp_client": mcp_client,
        "available_tools": state.get("available_tools", []),
        "first_llm_response": state["first_llm_response"],
        "parsed_tool_calls": parsed_tool_calls,
        "needs_tool_execution": state.get("needs_tool_execution", False),
        "tool_outputs": tool_outputs,  # æ›´æ–°å­—æ®µ
    }


############################################################################################################
def _build_tool_context(tool_outputs: List[Dict[str, Any]]) -> str:
    """
    æ„å»ºå·¥å…·æ‰§è¡Œç»“æœçš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²

    Args:
        tool_outputs: å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨

    Returns:
        str: æ ¼å¼åŒ–çš„å·¥å…·ç»“æœä¸Šä¸‹æ–‡
    """
    tool_context_parts = []
    for i, output in enumerate(tool_outputs, 1):
        tool_name = output.get("tool", "æœªçŸ¥å·¥å…·")
        success = output.get("success", False)
        result_data = output.get("result", "æ— ç»“æœ")
        exec_time = output.get("execution_time", 0.0)

        status = "æˆåŠŸ" if success else "å¤±è´¥"
        tool_context_parts.append(
            f"å·¥å…·{i}: {tool_name} (æ‰§è¡Œ{status}, è€—æ—¶{exec_time:.2f}s)\n"
            f"ç»“æœ: {result_data}"
        )

    return "\n\n".join(tool_context_parts)


############################################################################################################
async def _llm_re_invoke_node(state: McpState) -> McpState:
    """
    äºŒæ¬¡æ¨ç†èŠ‚ç‚¹ï¼šåŸºäºå·¥å…·ç»“æœé‡æ–°è°ƒç”¨ LLM

    messages å˜åŒ–ï¼š
    - è¿½åŠ  AIMessage(å·¥å…·ç»“æœ)
    - è¿½åŠ  HumanMessage(äºŒæ¬¡æ¨ç†æŒ‡ä»¤)
    - è¿½åŠ  AIMessage(re_invoke_response)

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        McpState: åŒ…å« re_invoke_response
    """
    tool_outputs = state.get("tool_outputs", [])

    # æ–­è¨€ï¼šæ­¤èŠ‚ç‚¹åªåº”åœ¨æœ‰å·¥å…·è¾“å‡ºæ—¶è¢«è°ƒç”¨
    assert tool_outputs, "äºŒæ¬¡æ¨ç†èŠ‚ç‚¹è¦æ±‚å¿…é¡»æœ‰å·¥å…·è¾“å‡º"

    # è¿›è¡ŒäºŒæ¬¡æ¨ç†
    llm = state["llm"]

    # æ„å»ºå·¥å…·ç»“æœä¸Šä¸‹æ–‡
    tool_context = _build_tool_context(tool_outputs)

    # æ‹†åˆ†æ¶ˆæ¯ï¼šAIMessage(å·¥å…·ç»“æœ) + HumanMessage(çº¦æŸå’Œè¦æ±‚)
    tool_result_message = AIMessage(content=tool_context)

    # ä½¿ç”¨é»˜è®¤äºŒæ¬¡æ¨ç†æŒ‡ä»¤æˆ–è‡ªå®šä¹‰æŒ‡ä»¤
    instruction_content = state.get("re_invoke_instruction")
    if instruction_content is None:
        re_invoke_instruction = HumanMessage(content=DEFAULT_RE_INVOKE_INSTRUCTION)
    else:
        re_invoke_instruction = instruction_content

    # ç›´æ¥ä¿®æ”¹ state["messages"]ï¼Œæ·»åŠ å·¥å…·ç»“æœå’ŒäºŒæ¬¡æ¨ç†æŒ‡ä»¤
    messages = state["messages"]
    messages.append(tool_result_message)
    messages.append(re_invoke_instruction)

    # äºŒæ¬¡è°ƒç”¨ LLMï¼ˆå¼‚å¸¸å‘ä¸Šä¼ æ’­ï¼‰
    # logger.debug("ğŸ”„ å¼€å§‹äºŒæ¬¡æ¨ç†ï¼ŒåŸºäºå·¥å…·ç»“æœç”Ÿæˆæ™ºèƒ½å›ç­”...")
    re_invoke_response = llm.invoke(messages)
    assert isinstance(
        re_invoke_response, AIMessage
    ), "äºŒæ¬¡æ¨ç†è¿”å›å¿…é¡»æ˜¯ AIMessage ç±»å‹"
    # logger.success("âœ… äºŒæ¬¡æ¨ç†å®Œæˆ")

    # å°†äºŒæ¬¡æ¨ç†å“åº”åŠ å…¥ messagesï¼Œä¿æŒå®Œæ•´é“¾è·¯
    messages.append(re_invoke_response)

    # âœ… ä¿æŒæ‰€æœ‰å¿…è¦å­—æ®µ
    return {
        "messages": messages,
        "llm": llm,
        "mcp_client": state["mcp_client"],
        "available_tools": state.get("available_tools", []),
        "tool_outputs": tool_outputs,
        "first_llm_response": state["first_llm_response"],
        "parsed_tool_calls": state.get("parsed_tool_calls", []),
        "needs_tool_execution": state.get("needs_tool_execution", False),
        "re_invoke_response": re_invoke_response,  # æ–°å¢å­—æ®µ
    }


############################################################################################################
def print_full_message_chain(state: McpState) -> None:
    """
    æ‰“å°å®Œæ•´çš„æ¶ˆæ¯é“¾è·¯ï¼Œç”¨äºè°ƒè¯•å’Œè¿½è¸ªå¯¹è¯æµç¨‹

    Args:
        state: å½“å‰çŠ¶æ€
    """
    messages = state.get("messages", [])
    logger.info(f"ğŸ“œ å®Œæ•´æ¶ˆæ¯é“¾è·¯ (å…± {len(messages)} æ¡æ¶ˆæ¯)")
    for i, msg in enumerate(messages, 0):
        logger.debug(
            f"[{i}] å®Œæ•´å†…å®¹:\n{msg.model_dump_json(indent=2, ensure_ascii=False)}\n"
        )


############################################################################################################
def _should_execute_tools(state: McpState) -> str:
    """
    æ¡ä»¶è·¯ç”±ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œå·¥å…·

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        str: "tool_execution" æˆ– "__end__"
    """
    needs_tool_execution = state.get("needs_tool_execution", False)
    return "tool_execution" if needs_tool_execution else "__end__"


############################################################################################################
def create_mcp_workflow() -> CompiledStateGraph[McpState, Any, McpState, McpState]:
    """
    åˆ›å»º MCP å·¥ä½œæµçŠ¶æ€å›¾

    å·¥ä½œæµï¼š
    preprocess â†’ llm_invoke â†’ tool_parse â†’ [æ¡ä»¶è·¯ç”±]
                                             â†“ (éœ€è¦å·¥å…·)
                                        tool_execution â†’ llm_re_invoke â†’ END
                                             â†“ (æ— éœ€å·¥å…·)
                                        END

    Returns:
        CompiledStateGraph: ç¼–è¯‘åçš„çŠ¶æ€å›¾
    """

    # æ„å»ºå¤šèŠ‚ç‚¹çŠ¶æ€å›¾
    graph_builder = StateGraph(McpState)

    # æ·»åŠ å„ä¸ªèŠ‚ç‚¹
    graph_builder.add_node("preprocess", _preprocess_node)
    graph_builder.add_node("llm_invoke", _llm_invoke_node)
    graph_builder.add_node("tool_parse", _tool_parse_node)
    graph_builder.add_node("tool_execution", _tool_execution_node)
    graph_builder.add_node("llm_re_invoke", _llm_re_invoke_node)

    # è®¾ç½®æµç¨‹è·¯å¾„
    graph_builder.set_entry_point("preprocess")
    graph_builder.add_edge("preprocess", "llm_invoke")
    graph_builder.add_edge("llm_invoke", "tool_parse")

    # æ¡ä»¶è·¯ç”±ï¼šå·¥å…·è§£æååˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œå·¥å…·
    graph_builder.add_conditional_edges(
        "tool_parse",
        _should_execute_tools,
        {
            "tool_execution": "tool_execution",  # éœ€è¦å·¥å…· â†’ å·¥å…·æ‰§è¡Œ
            "__end__": "__end__",  # æ— éœ€å·¥å…· â†’ ç›´æ¥ç»“æŸ
        },
    )

    # å·¥å…·æ‰§è¡Œåè¿›å…¥äºŒæ¬¡æ¨ç†ï¼Œç„¶åç»“æŸ
    graph_builder.add_edge("tool_execution", "llm_re_invoke")
    graph_builder.add_edge("llm_re_invoke", "__end__")

    return graph_builder.compile()  # type: ignore[return-value]


############################################################################################################
async def execute_mcp_workflow(
    work_flow: CompiledStateGraph[McpState, Any, McpState, McpState],
    context: List[BaseMessage],
    request: HumanMessage,
    llm: ChatDeepSeek,
    mcp_client: McpClient,
    re_invoke_instruction: Optional[HumanMessage] = None,
) -> List[BaseMessage]:
    """
    æ‰§è¡Œ MCP å·¥ä½œæµ

    è¿”å›é€»è¾‘ï¼š
    - æœ‰å·¥å…·æ‰§è¡Œ â†’ è¿”å› re_invoke_response
    - æ— å·¥å…·æ‰§è¡Œ â†’ è¿”å› first_llm_response

    Args:
        work_flow: å·²ç¼–è¯‘çš„çŠ¶æ€å›¾
        context: å†å²æ¶ˆæ¯åˆ—è¡¨
        request: ç”¨æˆ·å½“å‰è¾“å…¥
        llm: ChatDeepSeek å®ä¾‹
        mcp_client: MCP å®¢æˆ·ç«¯å®ä¾‹
        re_invoke_instruction: è‡ªå®šä¹‰äºŒæ¬¡æ¨ç†æŒ‡ä»¤ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å†…ç½®æ¨¡æ¿ï¼‰

    Returns:
        List[BaseMessage]: å“åº”æ¶ˆæ¯åˆ—è¡¨
    """
    ret: List[BaseMessage] = []

    # åœ¨å‡½æ•°å†…éƒ¨è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
    available_tools = await mcp_client.list_tools()
    if available_tools is None:
        available_tools = []

    # æ„é€  McpStateï¼ˆcontext + [request] åˆ›å»ºæ–°åˆ—è¡¨ï¼Œé¿å…ä¿®æ”¹ä¼ å…¥å‚æ•°ï¼‰
    workflow_state: McpState = {
        "messages": context + [request],
        "llm": llm,
        "mcp_client": mcp_client,
        "available_tools": available_tools,
        "tool_outputs": [],
        "re_invoke_instruction": re_invoke_instruction,  # ç›´æ¥ä¼ å…¥ï¼Œå¯èƒ½æ˜¯ None
    }

    try:

        # æœ€ç»ˆçŠ¶æ€
        last_state: Optional[McpState] = None

        # æµå¼å¤„ç†æ‰€æœ‰èŠ‚ç‚¹çš„æ›´æ–°
        async for event in work_flow.astream(workflow_state):
            for node_name, value in event.items():
                # æŒç»­æ›´æ–°çŠ¶æ€ï¼Œæœ€åä¸€ä¸ªå°±æ˜¯æœ€ç»ˆçŠ¶æ€
                last_state = value

        # æŒ‰é¡ºåºæ”¶é›†å“åº”ï¼š[first_llm_response, re_invoke_response]
        # å¤–éƒ¨ä½¿ç”¨ ret[-1] è·å–æœ€ç»ˆå“åº”
        if last_state:
            # 1. å…ˆæ·»åŠ ç¬¬ä¸€æ¬¡æ¨ç†ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            first_llm_response = last_state.get("first_llm_response")
            if first_llm_response:
                assert isinstance(
                    first_llm_response, AIMessage
                ), "first_llm_response å¿…é¡»æ˜¯ AIMessage ç±»å‹"
                ret.append(first_llm_response)
                # logger.debug("ğŸ“Œ å·²æ”¶é›† first_llm_response")

            # 2. å†æ·»åŠ äºŒæ¬¡æ¨ç†ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            re_invoke_response = last_state.get("re_invoke_response")
            if re_invoke_response:
                assert isinstance(
                    re_invoke_response, AIMessage
                ), "re_invoke_response å¿…é¡»æ˜¯ AIMessage ç±»å‹"
                ret.append(re_invoke_response)
                # logger.debug("ğŸ“Œ å·²æ”¶é›† re_invoke_response")

            # 3. æ—¥å¿—ï¼šæ˜ç¡®æœ€ç»ˆè¿”å›çš„æ˜¯å“ªä¸ª
            # if re_invoke_response:
            #     logger.debug(
            #         "âœ… è¿”å›é¡ºåº: [first_llm_response, re_invoke_response]ï¼Œä½¿ç”¨ ret[-1] è·å–äºŒæ¬¡æ¨ç†ç»“æœ"
            #     )
            # elif first_llm_response:
            #     logger.debug(
            #         "âœ… è¿”å›é¡ºåº: [first_llm_response]ï¼Œä½¿ç”¨ ret[-1] è·å–ç¬¬ä¸€æ¬¡æ¨ç†ç»“æœ"
            #     )
            # else:
            #     logger.error("âŒ æ— å¯ç”¨å“åº”ï¼Œè¿”å›ç©ºåˆ—è¡¨")

            # è°ƒè¯•ï¼šæ‰“å°å®Œæ•´æ¶ˆæ¯é“¾è·¯
            print_full_message_chain(last_state)

        else:
            logger.error("âŒ æœªè·å–åˆ°æœ€ç»ˆçŠ¶æ€")

    except Exception as e:
        logger.error(f"Stream processing error: {e}")

    return ret


############################################################################################################
