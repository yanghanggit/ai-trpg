#!/usr/bin/env python3
"""
Replicate æ–‡ç”Ÿå›¾å·¥å…·
ä¸€ä¸ªç®€å•æ˜“ç”¨çš„æ–‡ç”Ÿå›¾è„šæœ¬ï¼ŒåŒ…å«å®Œæ•´åŠŸèƒ½å’Œä½¿ç”¨ç¤ºä¾‹


# åŸºç¡€ä½¿ç”¨
python scripts/run_replicate_text2image.py "prompt text"

# æ¼”ç¤ºåŠŸèƒ½
python scripts/run_replicate_text2image.py --demo            # å•å¼ æ¼”ç¤º
python scripts/run_replicate_text2image.py --concurrent     # å¹¶å‘æ¼”ç¤º

# å®ç”¨åŠŸèƒ½
python scripts/run_replicate_text2image.py --test           # æµ‹è¯•è¿æ¥
python scripts/run_replicate_text2image.py --list-models    # æŸ¥çœ‹æ¨¡å‹
"""

import argparse
import asyncio
import os
import sys
from pathlib import Path
from typing import Any, Dict, Final

from magic_book.replicate import (
    test_replicate_api_connection,
    load_replicate_config,
    # get_default_generation_params,
    generate_and_download,
    generate_multiple_images,
)

# å…¨å±€å˜é‡
API_TOKEN: str = os.getenv("REPLICATE_API_TOKEN") or ""

replicate_config = load_replicate_config(Path("replicate_models.json"))

MODELS: Dict[str, Dict[str, str]] = replicate_config.image_models.model_dump(
    by_alias=True, exclude_none=True
)

DEFAULT_OUTPUT_DIR: Final[str] = "generated_images"


def _get_default_generation_params() -> Dict[str, Any]:
    """
    è·å–é»˜è®¤çš„å›¾ç‰‡ç”Ÿæˆå‚æ•°

    Returns:
        åŒ…å«é»˜è®¤å‚æ•°çš„å­—å…¸
    """
    return {
        "model_name": "sdxl-lightning",
        "negative_prompt": "worst quality, low quality, blurry",
        "width": 768,
        "height": 768,
        "num_inference_steps": 4,
        "guidance_scale": 7.5,
    }


async def run_demo() -> None:
    """è¿è¡Œæ¼”ç¤ºç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸ® Replicate æ–‡ç”Ÿå›¾æ¼”ç¤º")
    print("=" * 60)

    # 1. æµ‹è¯•è¿æ¥
    if not test_replicate_api_connection():
        print("âŒ è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®")
        return

    # 2. æŸ¥çœ‹å¯ç”¨æ¨¡å‹
    print("\nğŸ“‹ å¯ç”¨æ¨¡å‹:")
    for name, info in MODELS.items():
        cost = info["cost_estimate"]
        description = info["description"]
        print(f"  - {name}: {cost}")
        print(f"    {description}")

    # 3. ç”Ÿæˆæµ‹è¯•å›¾ç‰‡
    print("\nğŸ¨ ç”Ÿæˆæµ‹è¯•å›¾ç‰‡...")

    try:
        # å¿«é€Ÿæµ‹è¯• - ä½¿ç”¨æˆæœ¬æœ€ä½çš„æ¨¡å‹
        test_prompt = "a beautiful landscape with mountains and a lake"

        # è·å–é»˜è®¤å‚æ•°
        default_params = _get_default_generation_params()

        saved_path = await generate_and_download(
            prompt=test_prompt,
            model_name=default_params["model_name"],
            negative_prompt=default_params["negative_prompt"],
            width=default_params["width"],
            height=default_params["height"],
            num_inference_steps=default_params["num_inference_steps"],
            guidance_scale=default_params["guidance_scale"],
            output_dir=DEFAULT_OUTPUT_DIR,
            models_config=MODELS,
        )

        print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ! å›¾ç‰‡å·²ä¿å­˜åˆ°: {saved_path}")
        print("ğŸ’¡ æ‚¨å¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„å›¾ç‰‡ï¼Œç„¶åå°è¯•å…¶ä»–æç¤ºè¯")

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")


async def run_concurrent_demo() -> None:
    """è¿è¡Œå¹¶å‘ç”Ÿæˆæ¼”ç¤º"""
    print("=" * 60)
    print("ğŸš€ Replicate å¹¶å‘æ–‡ç”Ÿå›¾æ¼”ç¤º")
    print("=" * 60)

    # 1. æµ‹è¯•è¿æ¥
    if not test_replicate_api_connection():
        print("âŒ è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®")
        return

    # 2. å¤šä¸ªæç¤ºè¯
    prompts = [
        "peaceful mountain landscape",
        "ocean waves on sandy beach",
        "forest path in autumn",
    ]

    print(f"\nğŸ¨ å¹¶å‘ç”Ÿæˆ {len(prompts)} å¼ å›¾ç‰‡...")
    print("ğŸ“ æç¤ºè¯åˆ—è¡¨:")
    for i, prompt in enumerate(prompts, 1):
        print(f"  {i}. {prompt}")

    try:
        # è·å–é»˜è®¤å‚æ•°
        default_params = _get_default_generation_params()

        # å¹¶å‘ç”Ÿæˆ
        results = await generate_multiple_images(
            prompts=prompts,
            model_name="ideogram-v3-turbo",  # ä½¿ç”¨ç›¸å¯¹ç¨³å®šçš„æ¨¡å‹
            negative_prompt=default_params["negative_prompt"],
            width=512,  # ä½¿ç”¨è¾ƒå°å°ºå¯¸åŠ å¿«æµ‹è¯•
            height=512,
            num_inference_steps=default_params["num_inference_steps"],
            guidance_scale=default_params["guidance_scale"],
            output_dir=DEFAULT_OUTPUT_DIR,
            models_config=MODELS,
        )

        print(f"\nğŸ‰ å¹¶å‘ç”Ÿæˆå®Œæˆ! ç”Ÿæˆäº† {len(results)} å¼ å›¾ç‰‡:")
        for i, path in enumerate(results, 1):
            print(f"  {i}. {path}")
        print("ğŸ’¡ è¿™å±•ç¤ºäº†å¼‚æ­¥å¹¶å‘çš„å¼ºå¤§èƒ½åŠ›ï¼")

    except Exception as e:
        print(f"âŒ å¹¶å‘æ¼”ç¤ºå¤±è´¥: {e}")


async def main() -> None:
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""

    # æ£€æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®åŠ è½½
    if not MODELS:
        print("âŒ é”™è¯¯: å›¾åƒæ¨¡å‹é…ç½®æœªæ­£ç¡®åŠ è½½")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. replicate_models.json æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print("   2. JSON æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
        print("   3. image_models éƒ¨åˆ†æ˜¯å¦é…ç½®æ­£ç¡®")
        sys.exit(1)

    parser = argparse.ArgumentParser(description="Replicate æ–‡ç”Ÿå›¾å·¥å…·")

    # è·å–é»˜è®¤å‚æ•°
    default_params = _get_default_generation_params()

    parser.add_argument("prompt", nargs="?", help="æ–‡æœ¬æç¤ºè¯")
    parser.add_argument(
        "--model",
        "-m",
        default=default_params["model_name"],
        choices=list(MODELS.keys()),
        help=f"ä½¿ç”¨çš„æ¨¡å‹ (é»˜è®¤: {default_params['model_name']})",
    )
    parser.add_argument(
        "--negative",
        "-n",
        default=default_params["negative_prompt"],
        help="è´Ÿå‘æç¤ºè¯",
    )
    parser.add_argument(
        "--width",
        "-w",
        type=int,
        default=default_params["width"],
        help=f"å›¾ç‰‡å®½åº¦ (é»˜è®¤: {default_params['width']})",
    )
    parser.add_argument(
        "--height",
        type=int,
        default=default_params["height"],
        help=f"å›¾ç‰‡é«˜åº¦ (é»˜è®¤: {default_params['height']})",
    )
    parser.add_argument(
        "--size",
        choices=["small", "medium", "large", "wide", "tall"],
        help="é¢„è®¾å°ºå¯¸: small(512x512), medium(768x768), large(1024x1024), wide(1024x768), tall(768x1024)",
    )
    parser.add_argument(
        "--steps",
        "-s",
        type=int,
        default=default_params["num_inference_steps"],
        help="æ¨ç†æ­¥æ•°",
    )
    parser.add_argument(
        "--guidance",
        "-g",
        type=float,
        default=default_params["guidance_scale"],
        help="å¼•å¯¼æ¯”ä¾‹",
    )
    parser.add_argument("--output", "-o", default=DEFAULT_OUTPUT_DIR, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--list-models", action="store_true", help="åˆ—å‡ºå¯ç”¨æ¨¡å‹")
    parser.add_argument("--demo", action="store_true", help="è¿è¡Œæ¼”ç¤º")
    parser.add_argument("--concurrent", action="store_true", help="è¿è¡Œå¹¶å‘ç”Ÿæˆæ¼”ç¤º")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•è¿æ¥")

    args = parser.parse_args()

    try:
        print("âœ… Replicate å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

        # å¤„ç†é¢„è®¾å°ºå¯¸
        if args.size:
            size_presets = {
                "small": (512, 512),
                "medium": (768, 768),
                "large": (1024, 1024),
                "wide": (1024, 768),
                "tall": (768, 1024),
            }
            args.width, args.height = size_presets[args.size]
            print(f"ğŸ“ ä½¿ç”¨é¢„è®¾å°ºå¯¸ '{args.size}': {args.width}x{args.height}")

        # å¦‚æœæ˜¯è¿è¡Œæ¼”ç¤º
        if args.demo:
            await run_demo()
            return

        # å¦‚æœæ˜¯è¿è¡Œå¹¶å‘æ¼”ç¤º
        if args.concurrent:
            await run_concurrent_demo()
            return

        # å¦‚æœæ˜¯æµ‹è¯•è¿æ¥
        if args.test:
            test_replicate_api_connection()
            return

        # å¦‚æœåªæ˜¯åˆ—å‡ºæ¨¡å‹
        if args.list_models:
            print("ğŸ¨ å¯ç”¨æ¨¡å‹:")
            for name, info in MODELS.items():
                cost = info["cost_estimate"]
                description = info["description"]
                print(f"  - {name}: {cost}")
                print(f"    {description}")
            return

        # å¦‚æœæ²¡æœ‰æä¾›æç¤ºè¯ï¼Œæ˜¾ç¤ºå¸®åŠ©
        if not args.prompt:
            print("ğŸ¨ Replicate æ–‡ç”Ÿå›¾å·¥å…·")
            print("\nå¿«é€Ÿå¼€å§‹:")
            print(
                "  python run_replicate_text2image.py --demo            # è¿è¡Œå•å¼ æ¼”ç¤º"
            )
            print(
                "  python run_replicate_text2image.py --concurrent      # è¿è¡Œå¹¶å‘æ¼”ç¤º"
            )
            print("  python run_replicate_text2image.py --test            # æµ‹è¯•è¿æ¥")
            print(
                "  python run_replicate_text2image.py --list-models     # æŸ¥çœ‹å†…ç½®æ¨¡å‹"
            )
            print('  python run_replicate_text2image.py "ç”Ÿæˆä¸€åªçŒ«"       # ç”Ÿæˆå›¾ç‰‡')
            print("\nå°ºå¯¸é€‰é¡¹:")
            print("  --size small    # 512x512  (æœ€å¿«)")
            print("  --size medium   # 768x768  (æ¨è)")
            print("  --size large    # 1024x1024 (é«˜è´¨é‡)")
            print("  --size wide     # 1024x768 (æ¨ªå‘)")
            print("  --size tall     # 768x1024 (çºµå‘)")
            print("\nè¯¦ç»†å¸®åŠ©:")
            print("  python run_replicate_text2image.py -h")
            return

        # ç”Ÿæˆå¹¶ä¸‹è½½å›¾ç‰‡
        saved_path = await generate_and_download(
            prompt=args.prompt,
            model_name=args.model,
            negative_prompt=args.negative,
            width=args.width,
            height=args.height,
            num_inference_steps=args.steps,
            guidance_scale=args.guidance,
            output_dir=args.output,
            models_config=MODELS,
        )

        print(f"\nğŸ‰ å®Œæˆ! å›¾ç‰‡å·²ä¿å­˜åˆ°: {saved_path}")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
